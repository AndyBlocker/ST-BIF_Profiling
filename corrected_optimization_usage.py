#!/usr/bin/env python3
"""
ST-BIF SNN ä¼˜åŒ–ä½¿ç”¨ç¤ºä¾‹ - ä¿®å¤æ€§èƒ½é€€åŒ–é—®é¢˜

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•æ­£ç¡®ä½¿ç”¨ä¼˜åŒ–æ¥é¿å…æ€§èƒ½é€€åŒ–é—®é¢˜ã€‚

é—®é¢˜åŸå› åˆ†æ (Root Cause Analysis):
1. âŒ setup_optimizations() åœ¨ SNNWrapper åˆ›å»ºä¹‹åè°ƒç”¨ - Called after SNNWrapper creation
2. âŒ æ··åˆç²¾åº¦ä¸ST-BIFé˜ˆå€¼è®¡ç®—å†²çª - Mixed precision conflicts with ST-BIF thresholds  
3. âŒ è¿‡äºæ¿€è¿›çš„CUDAä¼˜åŒ–è®¾ç½® - Overly aggressive CUDA optimizations
4. âŒ å†…å­˜æ± ä¸ç°æœ‰tensorç®¡ç†å†²çª - Memory pool conflicts with existing tensor management

è§£å†³æ–¹æ¡ˆ (Solution):
âœ… åœ¨æ¨¡å‹åˆ›å»ºä¹‹å‰è®¾ç½®ä¼˜åŒ– - Setup optimizations BEFORE model creation
âœ… ä½¿ç”¨ST-BIFå…¼å®¹çš„ä¿å®ˆä¼˜åŒ– - Use ST-BIF compatible conservative optimizations
âœ… ç¦ç”¨ä¸å…¼å®¹çš„åŠŸèƒ½ - Disable incompatible features
âœ… æä¾›å›é€€æœºåˆ¶ - Provide fallback mechanisms
"""

import torch
import torch.nn as nn
import time
import sys
import os
from typing import Optional, Dict, Any

# Add paths for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import fixed optimization utilities
from snn.optimization_utils_fixed import setup_st_bif_optimizations, get_st_bif_optimizer, cleanup_st_bif_optimizations

def main():
    """ä¸»å‡½æ•°æ¼”ç¤ºæ­£ç¡®çš„ä¼˜åŒ–ä½¿ç”¨æ–¹å¼"""
    print("ğŸ”§ ST-BIF SNN Optimization - Corrected Usage Example")
    print("=" * 60)
    
    # æ­¥éª¤ 1: æ£€æŸ¥ç¯å¢ƒ
    print("\n1. ç¯å¢ƒæ£€æŸ¥ (Environment Check):")
    print(f"   CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name()}")
        print(f"   CUDA version: {torch.version.cuda}")
    
    # æ­¥éª¤ 2: æ­£ç¡®çš„ä¼˜åŒ–è®¾ç½®æ—¶æœº - BEFORE any model creation
    print("\n2. è®¾ç½®ä¼˜åŒ– (Setup Optimizations) - BEFORE model creation:")
    
    try:
        # è¿™æ˜¯å…³é”®ï¼šåœ¨ä»»ä½•æ¨¡å‹åˆ›å»ºä¹‹å‰è®¾ç½®ä¼˜åŒ–
        optimizer = setup_st_bif_optimizations(
            enable_memory_pool=True,     # å¯ç”¨å†…å­˜æ± 
            enable_tf32=True,            # å¯ç”¨TF32ï¼ˆå®‰å…¨ï¼‰
            enable_cudnn_benchmark=False, # ç¦ç”¨ï¼ˆé¿å…å˜é•¿è¾“å…¥é—®é¢˜ï¼‰
            memory_pool_size=3,          # ä¿å®ˆçš„æ± å¤§å°
            verbose=True
        )
        print("   âœ… Optimizations setup successful")
        
    except Exception as e:
        print(f"   âŒ Optimization setup failed: {e}")
        print("   ğŸ”„ Falling back to default settings...")
        optimizer = get_st_bif_optimizer()  # ä½¿ç”¨é»˜è®¤è®¾ç½®
    
    # æ­¥éª¤ 3: ç°åœ¨åˆ›å»ºæ¨¡å‹ (NOW create models)
    print("\n3. åˆ›å»ºæ¨¡å‹ (Create Models) - AFTER optimization setup:")
    
    # æ¨¡æ‹Ÿä½ çš„å®é™…ä½¿ç”¨åœºæ™¯
    model = create_test_model()
    print("   âœ… Base model created")
    
    # æ¨¡æ‹ŸSNNWrapperåˆ›å»º
    snn_model = simulate_snn_wrapper_creation(model, optimizer)
    print("   âœ… SNN wrapper created")
    
    # æ­¥éª¤ 4: æ€§èƒ½å¯¹æ¯”æµ‹è¯•
    print("\n4. æ€§èƒ½æµ‹è¯• (Performance Testing):")
    run_performance_comparison(model, snn_model, optimizer)
    
    # æ­¥éª¤ 5: ç›‘æ§å’Œè°ƒè¯•
    print("\n5. ä¼˜åŒ–çŠ¶æ€ç›‘æ§ (Optimization Status):")
    show_optimization_status(optimizer)
    
    # æ­¥éª¤ 6: æ¸…ç†
    print("\n6. æ¸…ç†èµ„æº (Cleanup):")
    cleanup_st_bif_optimizations()
    print("   âœ… Cleanup completed")

def create_test_model():
    """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
            self.fc = nn.Linear(64 * 8 * 8, 10)
            
        def forward(self, x):
            x = torch.relu(self.conv1(x))
            x = torch.max_pool2d(x, 2)
            x = torch.relu(self.conv2(x))  
            x = torch.max_pool2d(x, 2)
            x = x.view(x.size(0), -1)
            return self.fc(x)
    
    model = TestModel()
    if torch.cuda.is_available():
        model = model.cuda()
    return model

def simulate_snn_wrapper_creation(model, optimizer):
    """æ¨¡æ‹ŸSNNWrapperåˆ›å»ºè¿‡ç¨‹"""
    
    # è¿™é‡Œæ¨¡æ‹Ÿä½ çš„SNNWrapperåˆ›å»º
    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯ï¼š
    # snn_wrapper = SNNWrapper_MS(model, time_step=8, level=8, ...)
    
    class MockSNNWrapper(nn.Module):
        def __init__(self, base_model, optimizer_manager):
            super().__init__()
            self.base_model = base_model
            self.optimizer_manager = optimizer_manager
            self.memory_pool = optimizer_manager.get_memory_pool()
            self.T = 8  # æ—¶é—´æ­¥æ•°
            
        def forward(self, x):
            batch_size = x.size(0)
            
            # æ¨¡æ‹ŸST-BIFæ—¶é—´æ­¥å¤„ç†
            outputs = []
            for t in range(self.T):
                # ä½¿ç”¨å†…å­˜æ± ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.memory_pool:
                    temp_tensor = self.memory_pool.get_tensor((batch_size, 64))
                    # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
                    temp_result = temp_tensor + t * 0.1
                    self.memory_pool.return_tensor(temp_tensor)
                
                # åŸºç¡€å‰å‘ä¼ æ’­
                output = self.base_model(x)
                outputs.append(output)
            
            # èšåˆæ—¶é—´æ­¥è¾“å‡º
            return torch.stack(outputs).mean(dim=0)
    
    snn_wrapper = MockSNNWrapper(model, optimizer)
    return snn_wrapper

def run_performance_comparison(base_model, snn_model, optimizer):
    """è¿è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    
    batch_size = 16
    input_shape = (batch_size, 3, 32, 32)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    if torch.cuda.is_available():
        test_input = torch.randn(input_shape).cuda()
    else:
        test_input = torch.randn(input_shape)
    
    def benchmark_model(model, name, num_iterations=20):
        """åŸºå‡†æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        model.eval()
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(5):
                _ = model(test_input)
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        
        # è®¡æ—¶
        start_time = time.time()
        with torch.no_grad():
            for _ in range(num_iterations):
                output = model(test_input)
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        end_time = time.time()
        
        total_time = end_time - start_time
        avg_time = total_time / num_iterations
        throughput = batch_size * num_iterations / total_time
        
        print(f"   {name}:")
        print(f"     Avg time: {avg_time*1000:.2f}ms")
        print(f"     Throughput: {throughput:.1f} samples/sec")
        
        return avg_time, throughput
    
    # æµ‹è¯•åŸºç¡€æ¨¡å‹
    base_time, base_throughput = benchmark_model(base_model, "Base Model")
    
    # æµ‹è¯•SNNæ¨¡å‹  
    snn_time, snn_throughput = benchmark_model(snn_model, "SNN Model (with optimizations)")
    
    # è®¡ç®—æ¯”ç‡
    if base_time > 0:
        slowdown_ratio = snn_time / base_time
        print(f"   ğŸ“Š SNN vs Base slowdown: {slowdown_ratio:.2f}x")
        
        if slowdown_ratio < 10:  # å¦‚æœæ…¢äº10å€ï¼Œè¯´æ˜ä¼˜åŒ–æœ‰æ•ˆ
            print("   âœ… Optimization effective (reasonable slowdown)")
        else:
            print("   âš ï¸  High slowdown - consider disabling some optimizations")

def show_optimization_status(optimizer):
    """æ˜¾ç¤ºä¼˜åŒ–çŠ¶æ€"""
    status = optimizer.get_status()
    
    print("   ğŸ“‹ Current optimization status:")
    print(f"     Optimized: {status['is_optimized']}")
    print(f"     CUDA available: {status['cuda_available']}")
    
    if 'cuda_settings' in status:
        cuda_settings = status['cuda_settings']
        print(f"     cuDNN benchmark: {cuda_settings['cudnn_benchmark']}")
        print(f"     TF32 matmul: {cuda_settings['tf32_matmul']}")
    
    if 'memory_pool' in status:
        pool_stats = status['memory_pool']
        print(f"     Memory pool enabled: {pool_stats['enabled']}")
        print(f"     Memory pool hit rate: {pool_stats['hit_rate']:.2%}")
    
    if 'cuda_info' in status:
        cuda_info = status['cuda_info']
        print(f"     GPU memory allocated: {cuda_info['allocated_gb']:.2f} GB")

def demonstrate_fallback_pattern():
    """æ¼”ç¤ºå›é€€æ¨¡å¼å¤„ç†"""
    print("\n" + "=" * 60)
    print("å›é€€æ¨¡å¼æ¼”ç¤º (Fallback Pattern Demonstration)")
    print("=" * 60)
    
    print("""
å¦‚æœä¼˜åŒ–å¯¼è‡´æ€§èƒ½é€€åŒ–ï¼Œä½¿ç”¨ä»¥ä¸‹å›é€€ç­–ç•¥ï¼š

# ç­–ç•¥1: é€æ­¥ç¦ç”¨ä¼˜åŒ–åŠŸèƒ½
optimizer = setup_st_bif_optimizations(
    enable_memory_pool=False,  # é¦–å…ˆç¦ç”¨å†…å­˜æ± 
    enable_tf32=True,          # ä¿ç•™TF32
    enable_cudnn_benchmark=False
)

# ç­–ç•¥2: å®Œå…¨ç¦ç”¨ä¼˜åŒ–ï¼ˆå›åˆ°åŸå§‹æ€§èƒ½ï¼‰
optimizer = get_st_bif_optimizer()  # ä½¿ç”¨é»˜è®¤è®¾ç½®

# ç­–ç•¥3: è¿è¡Œæ—¶åˆ‡æ¢
memory_pool = optimizer.get_memory_pool()
if memory_pool:
    memory_pool.disable()  # è¿è¡Œæ—¶ç¦ç”¨å†…å­˜æ± 
    
# ç­–ç•¥4: æ€§èƒ½ç›‘æ§è‡ªåŠ¨è°ƒæ•´
if avg_time > expected_time * 1.5:  # å¦‚æœæ¯”é¢„æœŸæ…¢50%ä»¥ä¸Š
    optimizer.disable_memory_pool()  # è‡ªåŠ¨ç¦ç”¨é—®é¢˜åŠŸèƒ½
    print("âš ï¸  Auto-disabled memory pool due to performance regression")
""")

if __name__ == "__main__":
    # è¿è¡Œä¸»è¦ç¤ºä¾‹
    main()
    
    # æ˜¾ç¤ºå›é€€æ¨¡å¼
    demonstrate_fallback_pattern()
    
    print("\n" + "=" * 60)
    print("ğŸ¯ å…³é”®è¦ç‚¹æ€»ç»“ (Key Takeaways)")
    print("=" * 60)
    print("1. âœ… ä¼˜åŒ–å¿…é¡»åœ¨æ¨¡å‹åˆ›å»ºä¹‹å‰è®¾ç½®")
    print("   Optimizations MUST be setup BEFORE model creation")
    print()
    print("2. âœ… ä½¿ç”¨ä¿å®ˆçš„ä¼˜åŒ–è®¾ç½®é¿å…å†²çª") 
    print("   Use conservative optimization settings to avoid conflicts")
    print()
    print("3. âœ… æä¾›å›é€€æœºåˆ¶å¤„ç†æ€§èƒ½é€€åŒ–")
    print("   Provide fallback mechanisms for performance regressions")
    print()
    print("4. âœ… ç›‘æ§å’Œè°ƒè¯•ä¼˜åŒ–æ•ˆæœ")
    print("   Monitor and debug optimization effects")
    print()
    print("ğŸ”§ æ­£ç¡®çš„ä½¿ç”¨æ¨¡å¼:")
    print("   optimizer = setup_st_bif_optimizations()  # é¦–å…ˆè®¾ç½®")
    print("   model = load_model()                       # ç„¶ååˆ›å»ºæ¨¡å‹") 
    print("   snn_wrapper = SNNWrapper_MS(model, ...)   # æœ€ååˆ›å»ºwrapper")
    print()
    print("âŒ é”™è¯¯çš„ä½¿ç”¨æ¨¡å¼:")
    print("   model = load_model()                       # å…ˆåˆ›å»ºæ¨¡å‹")
    print("   snn_wrapper = SNNWrapper_MS(model, ...)   # å†åˆ›å»ºwrapper")  
    print("   setup_optimizations()                     # æœ€åè®¾ç½®ä¼˜åŒ– âŒ")