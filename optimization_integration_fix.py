#!/usr/bin/env python3
"""
ST-BIF SNN Optimization Integration Fix

This script demonstrates the CORRECT way to use optimizations to avoid performance degradation.
The key issues causing slowdown and their solutions:

é—®é¢˜åˆ†æ (Problem Analysis):
1. setup_optimizations() è°ƒç”¨æ—¶æœºé”™è¯¯ - Wrong timing of setup_optimizations()
2. å†…å­˜æ± ä¸ç°æœ‰tensorç®¡ç†å†²çª - Memory pool conflicts with existing tensor management  
3. æ··åˆç²¾åº¦ä¸ST-BIFç¥ç»å…ƒä¸å…¼å®¹ - Mixed precision incompatible with ST-BIF neurons
4. CUDAä¼˜åŒ–è®¾ç½®è¿‡äºæ¿€è¿› - CUDA optimizations too aggressive

è§£å†³æ–¹æ¡ˆ (Solutions):
1. ä¼˜åŒ–åº”è¯¥åœ¨æ¨¡å‹åˆ›å»ºä¹‹å‰è®¾ç½® - Set up optimizations BEFORE model creation
2. ä½¿ç”¨é€‰æ‹©æ€§ä¼˜åŒ–è€Œéå…¨éƒ¨ä¼˜åŒ– - Use selective optimizations instead of all
3. ST-BIFç‰¹å®šçš„ä¼˜åŒ–é…ç½® - ST-BIF specific optimization configuration
"""

import torch
import torch.nn as nn
import time
import sys
import os

# Add current directory to Python path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from snn.optimization_utils import TensorMemoryPool, CUDAOptimizer, MixedPrecisionTrainer, PerformanceMonitor

class SelectiveOptimizer:
    """
    Selective optimizer that applies only compatible optimizations for ST-BIF SNN.
    è§£å†³æ€§èƒ½é€€åŒ–çš„æ ¸å¿ƒç±» - Core class to fix performance degradation
    """
    
    def __init__(self):
        self.cuda_optimized = False
        self.memory_pool = None
        self.monitor = PerformanceMonitor()
        
    def setup_st_bif_optimizations(self, enable_memory_pool=True, enable_tf32=True, enable_cudnn_benchmark=True):
        """
        Setup optimizations specifically tuned for ST-BIF SNN to avoid performance degradation.
        
        å…³é”®ï¼šåªå¯ç”¨ä¸ST-BIFå…¼å®¹çš„ä¼˜åŒ– - Key: Only enable ST-BIF compatible optimizations
        
        Args:
            enable_memory_pool: Enable tensor memory pooling (é€šå¸¸æå‡æ€§èƒ½)
            enable_tf32: Enable TF32 for matrix operations (å¯¹ST-BIFå®‰å…¨)  
            enable_cudnn_benchmark: Enable cuDNN benchmark (éœ€è¦å›ºå®šè¾“å…¥å°ºå¯¸)
        """
        if not torch.cuda.is_available():
            print("âš ï¸  CUDA not available, skipping GPU optimizations")
            return self
            
        print("ğŸ”§ Setting up ST-BIF compatible optimizations...")
        
        # 1. Conservative CUDA settings - ä¿å®ˆçš„CUDAè®¾ç½®
        if enable_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            print("âœ… TF32 enabled for matrix operations")
        
        # 2. Conditional cuDNN benchmark - æ¡ä»¶æ€§cuDNNåŸºå‡†æµ‹è¯•
        if enable_cudnn_benchmark:
            # åªåœ¨è¾“å…¥å°ºå¯¸å›ºå®šæ—¶å¯ç”¨ - Only enable when input sizes are fixed
            torch.backends.cudnn.benchmark = True 
            print("âœ… cuDNN benchmark enabled (ensure fixed input sizes)")
        else:
            torch.backends.cudnn.benchmark = False
            print("âš ï¸  cuDNN benchmark disabled for variable input sizes")
            
        # 3. Memory pool with conservative settings - ä¿å®ˆçš„å†…å­˜æ± è®¾ç½®
        if enable_memory_pool:
            self.memory_pool = TensorMemoryPool()
            print("âœ… Memory pool initialized")
        
        # 4. ä¸å¯ç”¨æ··åˆç²¾åº¦ - Do NOT enable mixed precision for ST-BIF
        # Mixed precision can cause issues with ST-BIF threshold calculations
        print("âš ï¸  Mixed precision disabled (incompatible with ST-BIF thresholds)")
        
        # 5. Conservative memory management - ä¿å®ˆçš„å†…å­˜ç®¡ç†
        torch.cuda.empty_cache()
        
        self.cuda_optimized = True
        print("âœ… ST-BIF optimizations setup complete")
        return self
    
    def get_memory_pool(self):
        """Get memory pool instance if available."""
        return self.memory_pool
    
    def cleanup(self):
        """Clean up optimization resources."""
        if self.memory_pool:
            self.memory_pool.clear_pool()
        torch.cuda.empty_cache()
        self.monitor.reset()
        print("ğŸ§¹ Optimization cleanup complete")

def demonstrate_correct_usage():
    """
    æ¼”ç¤ºæ­£ç¡®çš„ä¼˜åŒ–ä½¿ç”¨æ–¹æ³• - Demonstrate correct optimization usage
    """
    print("=" * 60)
    print("ST-BIF SNN Optimization Integration - Correct Usage")
    print("=" * 60)
    
    # æ­¥éª¤1: åœ¨ä»»ä½•æ¨¡å‹åˆ›å»ºä¹‹å‰è®¾ç½®ä¼˜åŒ– - Step 1: Setup optimizations BEFORE any model creation
    print("\n1. Setting up optimizations BEFORE model creation...")
    optimizer = SelectiveOptimizer()
    
    # æ ¹æ®ä½ çš„ä½¿ç”¨æƒ…å†µè°ƒæ•´è¿™äº›å‚æ•° - Adjust these parameters based on your use case
    optimizer.setup_st_bif_optimizations(
        enable_memory_pool=True,      # é€šå¸¸å®‰å…¨ä¸”æœ‰æ•ˆ - Usually safe and effective
        enable_tf32=True,             # å¯¹ST-BIFå®‰å…¨ - Safe for ST-BIF  
        enable_cudnn_benchmark=False  # å¦‚æœè¾“å…¥å°ºå¯¸å˜åŒ–åˆ™ç¦ç”¨ - Disable if input sizes vary
    )
    
    # æ­¥éª¤2: åˆ›å»ºæ¨¡å‹ - Step 2: Create model
    print("\n2. Creating test model...")
    model = create_test_st_bif_model()
    
    # æ­¥éª¤3: æ€§èƒ½æµ‹è¯• - Step 3: Performance testing
    print("\n3. Running performance test...")
    test_performance_with_optimizations(model, optimizer)
    
    # æ­¥éª¤4: æ¸…ç† - Step 4: Cleanup
    print("\n4. Cleaning up...")
    optimizer.cleanup()

def create_test_st_bif_model():
    """Create a simple ST-BIF model for testing."""
    class SimpleSTBIFModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
            self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
            self.fc = nn.Linear(128 * 8 * 8, 10)
            
        def forward(self, x):
            x = torch.relu(self.conv1(x))
            x = torch.max_pool2d(x, 2)
            x = torch.relu(self.conv2(x))
            x = torch.max_pool2d(x, 2)
            x = x.view(x.size(0), -1)
            x = self.fc(x)
            return x
    
    model = SimpleSTBIFModel()
    if torch.cuda.is_available():
        model = model.cuda()
    return model

def test_performance_with_optimizations(model, optimizer):
    """Test model performance with optimizations."""
    batch_size = 32
    input_shape = (batch_size, 3, 32, 32)
    
    # Create test input
    if torch.cuda.is_available():
        x = torch.randn(input_shape).cuda()
        model = model.cuda()
    else:
        x = torch.randn(input_shape)
    
    # Warmup
    for _ in range(5):
        _ = model(x)
    
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    
    # Measure performance
    start_time = time.time()
    num_iterations = 50
    
    for i in range(num_iterations):
        with torch.no_grad():
            output = model(x)
        
        # ä½¿ç”¨å†…å­˜æ± ï¼ˆå¦‚æœå¯ç”¨ï¼‰- Use memory pool if available  
        if optimizer.memory_pool and i % 10 == 0:
            # ç¤ºä¾‹ï¼šè·å–å’Œè¿”å›ä¸´æ—¶tensor - Example: get and return temporary tensor
            temp_tensor = optimizer.memory_pool.get_tensor((batch_size, 128))
            optimizer.memory_pool.return_tensor(temp_tensor)
    
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    end_time = time.time()
    
    total_time = end_time - start_time
    avg_time_per_iter = total_time / num_iterations
    throughput = batch_size * num_iterations / total_time
    
    print(f"   Total time: {total_time:.4f}s")
    print(f"   Avg time per iteration: {avg_time_per_iter*1000:.2f}ms")
    print(f"   Throughput: {throughput:.1f} samples/sec")
    
    # Memory pool statistics
    if optimizer.memory_pool:
        stats = optimizer.memory_pool.get_stats()
        print(f"   Memory pool hit rate: {stats['hit_rate']:.2%}")

def integration_with_snn_wrapper_example():
    """
    å±•ç¤ºå¦‚ä½•ä¸SNNWrapperæ­£ç¡®é›†æˆ - Show how to correctly integrate with SNNWrapper
    """
    print("\n" + "=" * 60)  
    print("Integration with SNNWrapper - Correct Pattern")
    print("=" * 60)
    
    print("""
æ­£ç¡®çš„é›†æˆæ¨¡å¼ (Correct Integration Pattern):

# é”™è¯¯çš„æ–¹å¼ (WRONG WAY):
model = load_pretrained_model()
snn_wrapper = SNNWrapper_MS(model, ...)  # åˆ›å»ºwrapper
setup_optimizations()  # âŒ å¤ªæ™šäº†ï¼ä¼šå¯¼è‡´æ€§èƒ½é€€åŒ–

# æ­£ç¡®çš„æ–¹å¼ (CORRECT WAY):  
optimizer = SelectiveOptimizer()
optimizer.setup_st_bif_optimizations(...)  # âœ… é¦–å…ˆè®¾ç½®ä¼˜åŒ–

model = load_pretrained_model()
snn_wrapper = SNNWrapper_MS(model, ...)  # ç„¶ååˆ›å»ºwrapper

# åœ¨è®­ç»ƒ/æ¨ç†ä¸­ä½¿ç”¨å†…å­˜æ± 
memory_pool = optimizer.get_memory_pool()
if memory_pool:
    temp_tensor = memory_pool.get_tensor(shape)
    # ... use tensor ...
    memory_pool.return_tensor(temp_tensor)
""")

if __name__ == "__main__":
    # æ¼”ç¤ºæ­£ç¡®ç”¨æ³• - Demonstrate correct usage
    demonstrate_correct_usage()
    
    # æ˜¾ç¤ºé›†æˆç¤ºä¾‹ - Show integration example  
    integration_with_snn_wrapper_example()
    
    print("\n" + "=" * 60)
    print("è§£å†³æ–¹æ¡ˆæ€»ç»“ (Solution Summary):")
    print("=" * 60)
    print("1. âœ… åœ¨æ¨¡å‹åˆ›å»ºä¹‹å‰è®¾ç½®ä¼˜åŒ– - Setup optimizations BEFORE model creation")
    print("2. âœ… ä½¿ç”¨é€‰æ‹©æ€§ä¼˜åŒ–é¿å…ä¸å…¼å®¹ - Use selective optimizations to avoid incompatibilities") 
    print("3. âœ… ç¦ç”¨æ··åˆç²¾åº¦ï¼ˆä¸ST-BIFä¸å…¼å®¹ï¼‰- Disable mixed precision (incompatible with ST-BIF)")
    print("4. âœ… ä¿å®ˆçš„å†…å­˜æ± è®¾ç½® - Conservative memory pool settings")
    print("5. âœ… é€‚å½“çš„æ¸…ç†å’Œç›‘æ§ - Proper cleanup and monitoring")
    print("\nğŸ¯ å…³é”®ï¼šæ—¶æœºå’Œé€‰æ‹©æ€§æ˜¯é¿å…æ€§èƒ½é€€åŒ–çš„å…³é”®ï¼")
    print("   Key: Timing and selectivity are crucial to avoid performance degradation!")