#!/usr/bin/env python3
"""
Fixed SNN Optimization Utilities - ä¿®å¤ç‰ˆæœ¬

This module provides FIXED optimization utilities for ST-BIF SNN that avoid performance degradation.
ä¿®å¤äº†å¯¼è‡´æ€§èƒ½é€€åŒ–çš„å…³é”®é—®é¢˜ã€‚

Key fixes:
1. é€‰æ‹©æ€§ä¼˜åŒ–è€Œéå…¨é‡ä¼˜åŒ– - Selective optimizations instead of aggressive ones
2. ST-BIFå…¼å®¹çš„é…ç½® - ST-BIF compatible configurations  
3. æ­£ç¡®çš„æ—¶æœºæ§åˆ¶ - Proper timing control
4. ä¿å®ˆçš„å†…å­˜ç®¡ç† - Conservative memory management
"""

import torch
import torch.nn as nn
from typing import Dict, Tuple, Optional, Any, Union
import threading
import warnings
from contextlib import contextmanager

class CompatibleTensorMemoryPool:
    """
    ST-BIFå…¼å®¹çš„å†…å­˜æ±  - ST-BIF compatible memory pool
    æ›´ä¿å®ˆçš„è®¾ç½®ä»¥é¿å…ä¸ç°æœ‰tensorç®¡ç†å†²çª
    """
    
    def __init__(self, device='cuda', max_pool_size=5):
        self.device = torch.device(device)
        self.pool = {}
        self.lock = threading.Lock()
        self.hit_count = 0
        self.miss_count = 0
        self.max_pool_size = max_pool_size  # å‡å°‘æ± å¤§å°ä»¥é¿å…å†…å­˜é—®é¢˜
        self.enabled = True
        
    def enable(self):
        """å¯ç”¨å†…å­˜æ± """
        self.enabled = True
        
    def disable(self):
        """ç¦ç”¨å†…å­˜æ± ï¼ˆå›é€€åˆ°æ ‡å‡†åˆ†é…ï¼‰"""
        self.enabled = False
        self.clear_pool()
        
    def get_tensor(self, shape: Tuple[int, ...], dtype: torch.dtype = torch.float32, 
                   requires_grad: bool = False) -> torch.Tensor:
        """
        è·å–tensorï¼Œå¦‚æœç¦ç”¨åˆ™ç›´æ¥åˆ›å»ºæ–°çš„
        """
        if not self.enabled:
            return torch.zeros(shape, dtype=dtype, device=self.device, requires_grad=requires_grad)
            
        key = (shape, dtype, requires_grad)
        
        with self.lock:
            if key in self.pool and len(self.pool[key]) > 0:
                tensor = self.pool[key].pop()
                self.hit_count += 1
                tensor.zero_()
                return tensor
            else:
                self.miss_count += 1
                return torch.zeros(shape, dtype=dtype, device=self.device, requires_grad=requires_grad)
    
    def return_tensor(self, tensor: torch.Tensor):
        """
        è¿”å›tensoråˆ°æ± ä¸­ï¼Œå¦‚æœç¦ç”¨åˆ™å¿½ç•¥
        """
        if not self.enabled or tensor.device != self.device:
            return
            
        key = (tuple(tensor.shape), tensor.dtype, tensor.requires_grad)
        
        with self.lock:
            if key not in self.pool:
                self.pool[key] = []
            
            # ä½¿ç”¨æ›´å°çš„æ± å¤§å°
            if len(self.pool[key]) < self.max_pool_size:
                tensor = tensor.detach()
                if tensor.grad is not None:
                    tensor.grad = None
                self.pool[key].append(tensor)
    
    def clear_pool(self):
        """æ¸…ç©ºå†…å­˜æ± """
        with self.lock:
            self.pool.clear()
            self.hit_count = 0
            self.miss_count = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total_requests if total_requests > 0 else 0
        
        return {
            'enabled': self.enabled,
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': hit_rate,
            'pool_sizes': {str(k): len(v) for k, v in self.pool.items()}
        }

class ConservativeCUDAOptimizer:
    """
    ä¿å®ˆçš„CUDAä¼˜åŒ–å™¨ - Conservative CUDA optimizer
    åªåº”ç”¨ä¸ST-BIFå…¼å®¹çš„ä¼˜åŒ–
    """
    
    @staticmethod
    def setup_st_bif_cuda_optimizations(enable_tf32=True, enable_cudnn_benchmark=None, 
                                      memory_fraction=0.85):
        """
        ä¸ºST-BIFè®¾ç½®ä¿å®ˆçš„CUDAä¼˜åŒ–
        
        Args:
            enable_tf32: å¯ç”¨TF32ï¼ˆå¯¹ST-BIFå®‰å…¨ï¼‰
            enable_cudnn_benchmark: å¯ç”¨cuDNNåŸºå‡†æµ‹è¯•ï¼ˆNone=è‡ªåŠ¨æ£€æµ‹ï¼‰
            memory_fraction: GPUå†…å­˜ä½¿ç”¨æ¯”ä¾‹
        """
        if not torch.cuda.is_available():
            warnings.warn("CUDA not available, skipping optimizations")
            return
        
        optimizations_applied = []
        
        # 1. TF32ä¼˜åŒ–ï¼ˆå¯¹ST-BIFå®‰å…¨ï¼‰
        if enable_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            optimizations_applied.append("TF32 enabled")
        
        # 2. æ™ºèƒ½cuDNNåŸºå‡†æµ‹è¯•è®¾ç½®
        if enable_cudnn_benchmark is None:
            # è‡ªåŠ¨æ£€æµ‹ï¼šè®­ç»ƒæ—¶å¯ç”¨ï¼Œæ¨ç†æ—¶ç¦ç”¨
            enable_cudnn_benchmark = torch.is_grad_enabled()
            
        torch.backends.cudnn.benchmark = enable_cudnn_benchmark
        if enable_cudnn_benchmark:
            optimizations_applied.append("cuDNN benchmark enabled")
        else:
            optimizations_applied.append("cuDNN benchmark disabled (for variable inputs)")
        
        # 3. ä¿å®ˆçš„å†…å­˜è®¾ç½®
        try:
            if hasattr(torch.cuda, 'set_memory_fraction'):
                torch.cuda.set_memory_fraction(memory_fraction)
                optimizations_applied.append(f"Memory fraction: {memory_fraction}")
        except Exception as e:
            warnings.warn(f"Could not set memory fraction: {e}")
        
        # 4. æ¸…ç†ç°æœ‰ç¼“å­˜
        torch.cuda.empty_cache()
        optimizations_applied.append("Memory cache cleared")
        
        print("ğŸ”§ ST-BIF CUDA Optimizations Applied:")
        for opt in optimizations_applied:
            print(f"   âœ… {opt}")
    
    @staticmethod
    def get_memory_info() -> Dict[str, float]:
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if not torch.cuda.is_available():
            return {}
        
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        max_reserved = torch.cuda.max_memory_reserved() / 1024**3
        
        return {
            'allocated_gb': allocated,
            'reserved_gb': reserved, 
            'max_reserved_gb': max_reserved
        }
    
    @staticmethod
    def clear_memory_cache():
        """æ¸…ç†CUDAå†…å­˜ç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

class ST_BIF_OptimizationManager:
    """
    ST-BIFä¸“ç”¨ä¼˜åŒ–ç®¡ç†å™¨ - ST-BIF specific optimization manager
    è§£å†³æ€§èƒ½é€€åŒ–é—®é¢˜çš„æ ¸å¿ƒç±»
    """
    
    def __init__(self):
        self.memory_pool = None
        self.is_optimized = False
        self.optimization_config = {}
        
    def setup_optimizations(self, 
                          enable_memory_pool=True,
                          enable_tf32=True, 
                          enable_cudnn_benchmark=None,
                          memory_pool_size=3,
                          verbose=True):
        """
        è®¾ç½®ST-BIFå…¼å®¹çš„ä¼˜åŒ–
        
        Args:
            enable_memory_pool: å¯ç”¨å†…å­˜æ± 
            enable_tf32: å¯ç”¨TF32
            enable_cudnn_benchmark: å¯ç”¨cuDNNåŸºå‡†æµ‹è¯•ï¼ˆNone=è‡ªåŠ¨ï¼‰
            memory_pool_size: å†…å­˜æ± å¤§å°
            verbose: æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        if self.is_optimized:
            if verbose:
                print("âš ï¸  Optimizations already applied")
            return self
        
        if verbose:
            print("ğŸš€ Setting up ST-BIF compatible optimizations...")
        
        # 1. CUDAä¼˜åŒ–
        ConservativeCUDAOptimizer.setup_st_bif_cuda_optimizations(
            enable_tf32=enable_tf32,
            enable_cudnn_benchmark=enable_cudnn_benchmark
        )
        
        # 2. å†…å­˜æ± ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if enable_memory_pool:
            self.memory_pool = CompatibleTensorMemoryPool(max_pool_size=memory_pool_size)
            if verbose:
                print(f"   âœ… Memory pool initialized (max_size={memory_pool_size})")
        
        # ä¿å­˜é…ç½®
        self.optimization_config = {
            'memory_pool': enable_memory_pool,
            'tf32': enable_tf32,
            'cudnn_benchmark': enable_cudnn_benchmark,
            'pool_size': memory_pool_size
        }
        
        self.is_optimized = True
        
        if verbose:
            print("âœ… ST-BIF optimizations setup complete")
            print("âš ï¸  Note: Mixed precision disabled (incompatible with ST-BIF)")
            
        return self
    
    def get_memory_pool(self) -> Optional[CompatibleTensorMemoryPool]:
        """è·å–å†…å­˜æ± å®ä¾‹"""
        return self.memory_pool
    
    def enable_memory_pool(self):
        """å¯ç”¨å†…å­˜æ± """
        if self.memory_pool:
            self.memory_pool.enable()
            print("âœ… Memory pool enabled")
    
    def disable_memory_pool(self):
        """ç¦ç”¨å†…å­˜æ± ï¼ˆç”¨äºè°ƒè¯•æ€§èƒ½é—®é¢˜ï¼‰"""
        if self.memory_pool:
            self.memory_pool.disable()
            print("âš ï¸  Memory pool disabled")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–çŠ¶æ€"""
        status = {
            'is_optimized': self.is_optimized,
            'config': self.optimization_config.copy(),
            'cuda_available': torch.cuda.is_available()
        }
        
        if torch.cuda.is_available():
            status['cuda_info'] = ConservativeCUDAOptimizer.get_memory_info()
            status['cuda_settings'] = {
                'cudnn_benchmark': torch.backends.cudnn.benchmark,
                'tf32_matmul': torch.backends.cuda.matmul.allow_tf32,
                'tf32_cudnn': torch.backends.cudnn.allow_tf32
            }
        
        if self.memory_pool:
            status['memory_pool'] = self.memory_pool.get_stats()
            
        return status
    
    def cleanup(self):
        """æ¸…ç†ä¼˜åŒ–èµ„æº"""
        if self.memory_pool:
            self.memory_pool.clear_pool()
        ConservativeCUDAOptimizer.clear_memory_cache()
        print("ğŸ§¹ Optimization cleanup complete")

# å…¨å±€å®ä¾‹
_global_st_bif_optimizer = None

def get_st_bif_optimizer() -> ST_BIF_OptimizationManager:
    """è·å–å…¨å±€ST-BIFä¼˜åŒ–å™¨å®ä¾‹"""
    global _global_st_bif_optimizer
    if _global_st_bif_optimizer is None:
        _global_st_bif_optimizer = ST_BIF_OptimizationManager()
    return _global_st_bif_optimizer

# ä¾¿æ·å‡½æ•°
def setup_st_bif_optimizations(**kwargs):
    """
    ä¾¿æ·å‡½æ•°ï¼šè®¾ç½®ST-BIFä¼˜åŒ–
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    optimizer = setup_st_bif_optimizations(enable_memory_pool=True)
    """
    return get_st_bif_optimizer().setup_optimizations(**kwargs)

def cleanup_st_bif_optimizations():
    """æ¸…ç†ST-BIFä¼˜åŒ–"""
    get_st_bif_optimizer().cleanup()

def get_st_bif_memory_pool():
    """è·å–ST-BIFå†…å­˜æ± """
    return get_st_bif_optimizer().get_memory_pool()

if __name__ == "__main__":
    # æµ‹è¯•ä¿®å¤ç‰ˆæœ¬çš„ä¼˜åŒ–å·¥å…·
    print("Testing Fixed ST-BIF Optimization Utilities...")
    print("=" * 50)
    
    # 1. è®¾ç½®ä¼˜åŒ–
    optimizer = setup_st_bif_optimizations(
        enable_memory_pool=True,
        enable_tf32=True,
        memory_pool_size=3,
        verbose=True
    )
    
    # 2. æµ‹è¯•å†…å­˜æ± 
    memory_pool = get_st_bif_memory_pool()
    if memory_pool:
        print("\nğŸ“Š Testing memory pool...")
        tensor1 = memory_pool.get_tensor((100, 100))
        memory_pool.return_tensor(tensor1)
        tensor2 = memory_pool.get_tensor((100, 100))  # Should reuse
        
        print("Memory pool stats:", memory_pool.get_stats())
    
    # 3. æ˜¾ç¤ºçŠ¶æ€
    print("\nğŸ“‹ Optimization status:")
    status = optimizer.get_status()
    for key, value in status.items():
        print(f"   {key}: {value}")
    
    # 4. æ¸…ç†
    cleanup_st_bif_optimizations()
    print("\nâœ… Test completed successfully")