#!/usr/bin/env python3
# cuda_kernel_profiler_v3.2.py  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# æ¯”è¾ƒ ST-BIF CUDA kernelsï¼ˆorig vs. newï¼‰åœ¨å¤šç²¾åº¦ / timestep / feature
# ä¸Šçš„ forward & backward æ€§èƒ½ï¼›è¾“å‡º JSON ä¸ bar-plot PNGã€‚

from __future__ import annotations
import argparse, json, sys, time, warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torch.cuda.nvtx as nvtx

# â”€â”€â”€ Project path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT = Path(__file__).resolve().parents[2]          # è°ƒæ•´åˆ°ä½ çš„ repo å±‚çº§
sys.path.append(str(ROOT))

warnings.filterwarnings("ignore", category=UserWarning, module="cupy")

# â”€â”€â”€ å†…æ ¸å¯ç”¨æ€§æ£€æµ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from neuron_cupy.cuda_operator import ST_BIFNodeATGF_MS_CUDA as OriginalKernel
    original_available = True
except Exception as e:
    print(f"\033[33m[Warn] åŸå§‹å†…æ ¸ä¸å¯ç”¨: {e}\033[0m")
    original_available = False

try:
    from neuron_cupy.cuda_operator_new import ST_BIFNodeATGF_MS_CUDA as NewKernel
    new_available = True
except Exception as e:
    print(f"\033[33m[Warn] æ–°å†…æ ¸ä¸å¯ç”¨: {e}\033[0m")
    new_available = False

from snn.neurons.st_bif_neurons import ST_BIFNeuron_MS


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class CUDAKernelProfiler:
    _ATOL = {"fp64": 1e-7, "fp32": 1e-5, "fp16": 1e-3}
    _RTOL = {"fp64": 1e-5, "fp32": 1e-4, "fp16": 1e-2}
    _DTYPE = {"fp64": torch.float64, "fp32": torch.float32, "fp16": torch.float16}

    def __init__(self, bs: int, runs: int, feats: List[int], ts_list: List[int],
                 precisions: List[str], include_backward: bool, out_dir: Path):
        if not torch.cuda.is_available():
            raise RuntimeError("éœ€è¦ CUDA è®¾å¤‡")

        self.bs, self.runs = bs, runs
        self.feats, self.ts_list = feats, ts_list
        self.precisions = precisions
        self.do_backward = include_backward
        self.dev = "cuda"
        self.out_dir = out_dir

        self._data_cache: Dict[tuple[int, torch.dtype], torch.Tensor] = {}
        self.records: List[Dict] = []

        self.kernels = []
        if original_available:
            self.kernels.append("orig")
        if new_available:
            self.kernels.append("new")
        if not self.kernels:
            raise RuntimeError("æ‰¾ä¸åˆ°å¯ç”¨å†…æ ¸")

    # â”€â”€â”€ è¾“å…¥ç¼“å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _get_data(self, F: int, dtype: torch.dtype, max_T: int):
        key = (F, dtype)
        if key not in self._data_cache:
            torch.manual_seed(42)
            self._data_cache[key] = 10 * torch.randn(self.bs * max_T, F,
                                                device=self.dev, dtype=dtype)
        return self._data_cache[key]

    # â”€â”€â”€ æ„é€ ç¥ç»å…ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _build_neuron(self, T: int, dtype: torch.dtype, use_new: bool):
        n = ST_BIFNeuron_MS(torch.tensor(0.5, dtype=dtype, device=self.dev),
                            level=8, sym=True, first_neuron=True).to(dtype)
        n.T = T
        
        # ä¿®å¤ï¼šç¡®ä¿pos_maxå’Œneg_minä¹Ÿè½¬æ¢ä¸ºæ­£ç¡®çš„dtype
        n.pos_max = n.pos_max.to(dtype)
        n.neg_min = n.neg_min.to(dtype)
        n.prefire = n.prefire.to(dtype)

        if use_new:
            import types

            def fwd(self, x):
                N = x.size(0)
                # ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ˜¯æ­£ç¡®çš„dtype
                spk, _, _ = NewKernel.apply(
                    x.view(T, N // T, -1).flatten(2),
                    self.q_threshold, 
                    self.pos_max.to(dtype), 
                    self.neg_min.to(dtype), 
                    self.prefire.to(dtype)
                )
                return spk.view_as(x) * self.q_threshold

            n.forward = types.MethodType(fwd, n)

        return n.eval().to(self.dev)

    # â”€â”€â”€ ç­‰æ•ˆæ€§æ£€æŸ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _equiv(self, T: int, F: int, prec: str):
        if not (original_available and new_available):
            return True  # åªæœ‰ä¸€ä¸ªå†…æ ¸æ—¶è®¤ä¸ºç›¸åŒ

        dtype = self._DTYPE[prec]
        atol, rtol = self._ATOL[prec], self._RTOL[prec]

        x = self._get_data(F, dtype, T)[:self.bs * T].clone()

        a = self._build_neuron(T, dtype, use_new=False)
        b = self._build_neuron(T, dtype, use_new=True)

        with torch.inference_mode():
            a.reset(); oa = a(x)
            b.reset(); ob = b(x)

        diff = (oa - ob).abs()
        md, mean = diff.max().item(), diff.mean().item()
        ok = md <= atol or md <= rtol * oa.abs().max().item()
        tag = "\033[32mâœ“" if ok else "\033[31mâœ—"
        print(f"      ç­‰æ•ˆæ€§ {tag} (max={md:.2e}, mean={mean:.2e})")
        
        # å¦‚æœä¸ç›¸åŒï¼Œè¾“å‡ºè¯¦ç»†å¯¹æ¯”ä¿¡æ¯
        if not ok:
            print(f"\nğŸš¨ é¦–æ¬¡å‘ç°ä¸ç­‰æ•ˆç»“æœ! T={T}, F={F}, ç²¾åº¦={prec}")
            print(f"å·®å¼‚ç»Ÿè®¡:")
            print(f"  â€¢ æœ€å¤§å·®å¼‚: {md:.6e}")
            print(f"  â€¢ å¹³å‡å·®å¼‚: {mean:.6e}")
            print(f"  â€¢ å®¹å·®é˜ˆå€¼: atol={atol:.2e}, rtol={rtol:.2e}")
            
            print(x.dtype)
            
            # ç»Ÿè®¡ä¸åŒçš„æ•°é‡è€Œéå‡å€¼
            total_elements = oa.numel()
            different_elements = (diff > atol).sum().item()
            different_ratio = different_elements / total_elements * 100
            
            print(f"  â€¢ ä¸åŒå…ƒç´ æ•°é‡: {different_elements}/{total_elements} ({different_ratio:.2f}%)")
            print(f"  â€¢ åŸå§‹ç»“æœç»Ÿè®¡: min={oa.min():.6e}, max={oa.max():.6e}, std={oa.std():.6e}")
            print(f"  â€¢ æ–°ç‰ˆç»“æœç»Ÿè®¡: min={ob.min():.6e}, max={ob.max():.6e}, std={ob.std():.6e}")
            
            # è¾“å‡ºå‰å‡ ä¸ªä¸åŒçš„å€¼ç¤ºä¾‹
            flat_oa = oa.flatten()
            flat_ob = ob.flatten()
            flat_diff = diff.flatten()
            
            # æ‰¾åˆ°å·®å¼‚æœ€å¤§çš„å‰10ä¸ªä½ç½®
            _, indices = torch.topk(flat_diff, min(10, total_elements))
            print(f"\nå‰10ä¸ªæœ€å¤§å·®å¼‚ä½ç½®:")
            print("    ç´¢å¼•      åŸå§‹å€¼      æ–°ç‰ˆå€¼      å·®å¼‚å€¼")
            print("    " + "-" * 50)
            for i, idx in enumerate(indices):
                orig_val = flat_oa[idx].item()
                new_val = flat_ob[idx].item()
                diff_val = flat_diff[idx].item()
                print(f"    {idx:6d}   {orig_val:10.6e}  {new_val:10.6e}  {diff_val:10.6e}")
            
            return False  # æ ‡è®°ä¸ºä¸ç›¸åŒ
            
        return True  # ç›¸åŒ

    # â”€â”€â”€ Profile å• kernel / direction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _profile(self, T: int, F: int, prec: str, kernel: str, direction: str):
        dtype = self._DTYPE[prec]
        use_new = (kernel == "new")
        x_full = self._get_data(F, dtype, T)
        x = x_full[:self.bs * T].clone().requires_grad_(direction == "backward")

        n = self._build_neuron(T, dtype, use_new)

        # é¢„çƒ­
        with torch.inference_mode():
            for _ in range(4):
                n.reset(); _ = n(x)
        if direction == "backward":
            n.reset(); (n(x)).sum().backward(); torch.cuda.synchronize()

        torch.cuda.synchronize()
        times = []
        memory_peaks = []
        
        for i in range(self.runs):
            nvtx.range_push(f"{kernel}_{direction}_{i}")
            n.reset()
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # è®°å½•æ‰§è¡Œå‰å†…å­˜
            mem_before = torch.cuda.memory_allocated()
            t0 = time.perf_counter()

            if direction == "forward":
                with torch.inference_mode():
                    _ = n(x)
            else:
                (n(x)).sum().backward()

            torch.cuda.synchronize()
            # è®°å½•æ‰§è¡Œåå†…å­˜å³°å€¼
            mem_peak = torch.cuda.max_memory_allocated()
            memory_peaks.append(mem_peak - mem_before)
            torch.cuda.reset_peak_memory_stats()
            
            times.append((time.perf_counter() - t0) * 1e3)
            nvtx.range_pop()

        arr = np.asarray(times)
        mem_arr = np.asarray(memory_peaks)
        mean_ms = arr.mean().item()
        mean_mem_mb = mem_arr.mean().item() / (1024**2)  # è½¬æ¢ä¸ºMB

        print(f"      {kernel:<4s}-{direction[0]} : {mean_ms:7.2f} ms "
              f"({mean_mem_mb:6.1f} MB)")

        self.records.append(dict(
            precision=prec, kernel=kernel, direction=direction,
            T=T, F=F, metric="mean_ms", value=mean_ms
        ))
        self.records.append(dict(
            precision=prec, kernel=kernel, direction=direction,
            T=T, F=F, metric="memory_mb", value=mean_mem_mb
        ))

    # â”€â”€â”€ ä¸»å¾ªç¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def run(self):
        print(f"\nCUDA Kernel Profiler â”‚ device: {torch.cuda.get_device_name(0)}")
        print(f"bs={self.bs}, runs={self.runs}, precisions={self.precisions}, "
              f"Ts={self.ts_list}, Fs={self.feats}, backward={self.do_backward}\n")

        first_difference_found = False
        
        for prec in self.precisions:
            if first_difference_found:
                break
                
            print(f"â”€â”€ ç²¾åº¦ {prec} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
            for T in self.ts_list:
                if first_difference_found:
                    break
                    
                for F in self.feats:
                    if first_difference_found:
                        break
                        
                    print(f"    T={T:2d}, F={F:4d}")
                    is_equivalent = self._equiv(T, F, prec)
                    
                    # è¿è¡Œæ€§èƒ½æµ‹è¯•
                    for kernel in self.kernels:
                        self._profile(T, F, prec, kernel, "forward")
                        if self.do_backward:
                            self._profile(T, F, prec, kernel, "backward")
                    
                    # å¦‚æœå‘ç°ä¸ç­‰æ•ˆï¼Œè¾“å‡ºå®Œæ•´ç»“æœåé€€å‡º
                    if not is_equivalent and len(self.kernels) >= 2:
                        print(f"\nğŸ¯ åœ¨é¦–ä¸ªä¸ç­‰æ•ˆbenchmark (T={T}, F={F}, {prec}) å¤„åœæ­¢")
                        print(f"ğŸ“Š æ­£åœ¨ä¿å­˜å½“å‰ç»“æœ...")
                        first_difference_found = True
                        break
        
        if not first_difference_found:
            print(f"\nâœ… æ‰€æœ‰æµ‹è¯•é…ç½®éƒ½ç­‰æ•ˆ!")
        
        print(f"\nğŸ“ˆ æ€»å…±æ”¶é›†äº† {len(self.records)} æ¡æ€§èƒ½è®°å½•")

    # â”€â”€â”€ ä¿å­˜ JSON + barplot PNG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # â”€â”€â”€ ä¿å­˜ JSON + barplot PNG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def save(self):
        import json, pandas as pd, matplotlib.pyplot as plt

        df = pd.DataFrame(self.records)
        js = self.out_dir / "kernel_prof_full.json"
        js.write_text(json.dumps(df.to_dict("records"), indent=2))
        print(f"\nå®Œæ•´ç»“æœå·²ä¿å­˜: {js}")

        if len(self.kernels) < 2:
            print("ä»…æ£€æµ‹åˆ°ä¸€ä¸ªå†…æ ¸ï¼Œè·³è¿‡ barplot å¯¹æ¯”")
            return

        # -- ç»˜åˆ¶1-2å¼ ç»¼åˆå¯¹æ¯”å›¾ ----------------------------
        # å›¾1: Latency å¯¹æ¯” (æ‰€æœ‰ç²¾åº¦å’Œæ–¹å‘)
        latency_data = df[df["metric"] == "mean_ms"]
        if not latency_data.empty:
            fig, axes = plt.subplots(1, len(self.precisions), figsize=(6*len(self.precisions), 5))
            if len(self.precisions) == 1:
                axes = [axes]
            
            for i, prec in enumerate(self.precisions):
                sub_prec = latency_data[latency_data["precision"] == prec]
                if sub_prec.empty:
                    continue
                    
                # ç»„åˆæ ‡ç­¾ï¼šT?-F?-æ–¹å‘
                sub_prec = sub_prec.copy()
                sub_prec["cfg"] = ("T" + sub_prec["T"].astype(str) + 
                                  "-F" + sub_prec["F"].astype(str) + 
                                  "-" + sub_prec["direction"].str[:3])
                
                # é€è§†è¡¨
                pivot = (sub_prec.pivot_table(index="cfg", columns="kernel", values="value")
                        .reindex(sorted(sub_prec["cfg"].unique())))
                
                pivot.plot(kind="bar", ax=axes[i])
                axes[i].set_title(f"{prec.upper()} Latency (ms)")
                axes[i].set_xlabel("T-F-æ–¹å‘")
                axes[i].set_ylabel("Latency (ms)")
                axes[i].grid(True, linestyle="--", alpha=.4)
                axes[i].tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            plt.savefig(self.out_dir / "latency_comparison.png", dpi=120)
            plt.close()

        # å›¾2: GPU Memory å¯¹æ¯” (æ‰€æœ‰ç²¾åº¦å’Œæ–¹å‘)
        memory_data = df[df["metric"] == "memory_mb"]
        if not memory_data.empty:
            fig, axes = plt.subplots(1, len(self.precisions), figsize=(6*len(self.precisions), 5))
            if len(self.precisions) == 1:
                axes = [axes]
            
            for i, prec in enumerate(self.precisions):
                sub_prec = memory_data[memory_data["precision"] == prec]
                if sub_prec.empty:
                    continue
                    
                # ç»„åˆæ ‡ç­¾ï¼šT?-F?-æ–¹å‘
                sub_prec = sub_prec.copy()
                sub_prec["cfg"] = ("T" + sub_prec["T"].astype(str) + 
                                  "-F" + sub_prec["F"].astype(str) + 
                                  "-" + sub_prec["direction"].str[:3])
                
                # é€è§†è¡¨
                pivot = (sub_prec.pivot_table(index="cfg", columns="kernel", values="value")
                        .reindex(sorted(sub_prec["cfg"].unique())))
                
                pivot.plot(kind="bar", ax=axes[i])
                axes[i].set_title(f"{prec.upper()} GPU Memory (MB)")
                axes[i].set_xlabel("T-F-æ–¹å‘")
                axes[i].set_ylabel("Memory Usage (MB)")
                axes[i].grid(True, linestyle="--", alpha=.4)
                axes[i].tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            plt.savefig(self.out_dir / "memory_comparison.png", dpi=120)
            plt.close()

        print(f"å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: latency_comparison.png, memory_comparison.png")

# â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse():
    p = argparse.ArgumentParser(
        description="Profile orig/new ST-BIF CUDA kernels with multi-precision."
    )
    p.add_argument("--bs", type=int, default=32)
    p.add_argument("--runs", type=int, default=50)
    p.add_argument("--features", nargs="+", type=int, default=[256, 512, 1024, 2048, 4096])
    p.add_argument("--ts", nargs="+", type=int, default=[1, 2, 4, 8, 16, 32])
    p.add_argument("--precisions", nargs="+", choices=["fp64", "fp32", "fp16"],
                   default=["fp64", "fp32", "fp16"])
    p.add_argument("--no_backward", action="store_true",
                   help="Skip backward profiling")
    return p.parse_args()


def main():
    args = parse()
    ts_name = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_dir = ROOT / "outputs" / "nsys_results" / ts_name
    out_dir.mkdir(parents=True, exist_ok=True)

    profiler = CUDAKernelProfiler(
        bs=args.bs, runs=args.runs,
        feats=args.features, ts_list=args.ts,
        precisions=args.precisions,
        include_backward=not args.no_backward,
        out_dir=out_dir
    )
    profiler.run()
    profiler.save()


if __name__ == "__main__":
    main()
