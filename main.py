import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR
from torch.amp.autocast_mode import autocast
from torch.amp.grad_scaler import GradScaler
from spike_quan_wrapper_ICML import SNNWrapper_MS, myquan_replace_resnet
import resnet
from torchvision import datasets, transforms
import os
import time
import math
import multiprocessing as mp
import json
import glob
from datetime import datetime
from tqdm import tqdm

# build CIFAR10 dataset in /home/zilingwei/cifar10

def build_dataset():
    # CIFAR10çš„æ ‡å‡†å‡å€¼å’Œæ ‡å‡†å·®
    mean = [0.4914, 0.4822, 0.4465]
    std = [0.2023, 0.1994, 0.2010]
    
    # è®­ç»ƒæ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),  # éšæœºè£å‰ª+padding
        transforms.RandomHorizontalFlip(p=0.5),  # éšæœºæ°´å¹³ç¿»è½¬
        transforms.RandomRotation(degrees=15),  # éšæœºæ—‹è½¬
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # é¢œè‰²æŠ–åŠ¨
        transforms.ToTensor(),
        transforms.Normalize(mean, std),
        transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3))  # éšæœºæ“¦é™¤
    ])
    
    # æµ‹è¯•æ•°æ®é¢„å¤„ç†ï¼ˆä¸ä½¿ç”¨å¢å¼ºï¼‰
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
    
    trainset = datasets.CIFAR10(root='/home/zilingwei/cifar10', train=True, download=True, transform=train_transform)
    testset = datasets.CIFAR10(root='/home/zilingwei/cifar10', train=False, download=True, transform=test_transform)
    return trainset, testset

def find_optimal_batch_size(model, device, dataset, min_batch_size=32, max_batch_size=2048):
    """
    è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜batch size - é€šè¿‡å®é™…è®­ç»ƒæ­¥éª¤æµ‹è¯•
    """
    print("ğŸ” è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜batch size...")
    
    if not torch.cuda.is_available():
        print(f"âš ï¸  æœªæ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨é»˜è®¤batch size: {min_batch_size}")
        return min_batch_size
    
    optimal_batch_size = min_batch_size
    model_clone = None
    
    # åˆ›å»ºä¸€ä¸ªæ¨¡å‹å‰¯æœ¬ç”¨äºæµ‹è¯•
    try:
        import copy
        model_clone = copy.deepcopy(model)
        model_clone.to(device)
        model_clone.train()
    except:
        print("âš ï¸  æ— æ³•åˆ›å»ºæ¨¡å‹å‰¯æœ¬ï¼Œä½¿ç”¨åŸæ¨¡å‹æµ‹è¯•")
        model_clone = model
    
    criterion = nn.CrossEntropyLoss()
    
    for batch_size in [32, 64, 128, 256, 512, 768, 1024, 1536, 2048]:
        if batch_size > max_batch_size:
            break
            
        try:
            print(f"ğŸ“Š æµ‹è¯•batch size: {batch_size}")
            
            # æ¸…ç©ºæ˜¾å­˜ç¼“å­˜
            torch.cuda.empty_cache()
            
            # è®°å½•æ¸…ç©ºåçš„æ˜¾å­˜åŸºçº¿
            torch.cuda.reset_peak_memory_stats()
            baseline_memory = torch.cuda.memory_allocated()
            
            # åˆ›å»ºä¸´æ—¶æ•°æ®åŠ è½½å™¨
            temp_loader = torch.utils.data.DataLoader(
                dataset, batch_size=batch_size, shuffle=True, num_workers=0  # é¿å…å¤šè¿›ç¨‹å¹²æ‰°
            )
            
            # åˆ›å»ºä¼˜åŒ–å™¨
            optimizer = optim.AdamW(model_clone.parameters(), lr=1e-4)
            
            # æ‰§è¡Œå‡ ä¸ªå®Œæ•´çš„è®­ç»ƒæ­¥éª¤æ¥å‡†ç¡®æµ‹è¯•æ˜¾å­˜ä½¿ç”¨
            for i, (data, target) in enumerate(temp_loader):
                if i >= 3:  # æµ‹è¯•3ä¸ªbatchå°±å¤Ÿäº†
                    break
                    
                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                output = model_clone(data)
                loss = criterion(output, target)
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                # è®°å½•å³°å€¼æ˜¾å­˜ä½¿ç”¨
                current_memory = torch.cuda.memory_allocated()
                peak_memory = torch.cuda.max_memory_allocated()
            
            # è®¡ç®—å®é™…ä½¿ç”¨çš„æ˜¾å­˜
            actual_usage = (peak_memory - baseline_memory) / 1024**3  # GB
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            memory_ratio = peak_memory / (memory_total * 1024**3)  # æ€»æ˜¾å­˜æ¯”ä¾‹
            
            print(f"   - æ˜¾å­˜ä½¿ç”¨: {actual_usage:.2f}GB (å³°å€¼: {peak_memory/1024**3:.2f}GB/{memory_total:.2f}GB, {memory_ratio:.1%})")
            
            # å¦‚æœæ˜¾å­˜ä½¿ç”¨ç‡è¶…è¿‡85%ï¼Œåœæ­¢å¢åŠ batch size
            if memory_ratio > 0.85:
                print(f"   - âš ï¸  æ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œåœæ­¢æµ‹è¯•")
                break
            
            # å¦‚æœactual_usageè¿‡å°ï¼Œè¯´æ˜batch sizeè¿˜å¯ä»¥ç»§ç»­å¢åŠ 
            if actual_usage < 0.1:  # å°äº100MBè¯´æ˜è¿˜æœ‰å¾ˆå¤§ç©ºé—´
                optimal_batch_size = batch_size
                continue
            
            optimal_batch_size = batch_size
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¶³å¤Ÿç©ºé—´è¿›ä¸€æ­¥å¢åŠ 
            if memory_ratio > 0.7:  # è¶…è¿‡70%å°±æ¯”è¾ƒä¿å®ˆ
                print(f"   - âœ… æ˜¾å­˜ä½¿ç”¨é€‚ä¸­ï¼Œé€‰æ‹©æ­¤batch size")
                break
                
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"   - âŒ æ˜¾å­˜ä¸è¶³ï¼Œå›é€€åˆ°ä¸Šä¸€ä¸ªbatch size")
                break
            else:
                print(f"   - âŒ æµ‹è¯•å‡ºé”™: {e}")
                raise e
        finally:
            torch.cuda.empty_cache()
    
    # ä¸ºäº†ä¿é™©èµ·è§ï¼Œæœ€ç»ˆé€‰æ‹©çš„batch sizeç¨å¾®ä¿å®ˆä¸€ç‚¹
    if optimal_batch_size >= 256:
        optimal_batch_size = int(optimal_batch_size * 0.8)  # å‡å°‘20%
        optimal_batch_size = max(optimal_batch_size, 128)  # ä½†ä¸ä½äº128
    
    print(f"ğŸ¯ æœ€ç»ˆé€‰æ‹©batch size: {optimal_batch_size}")
    return optimal_batch_size

train_set, test_set = build_dataset()

# é¦–å…ˆåˆ›å»ºæ¨¡å‹æ¥æµ‹è¯•batch size
temp_model = resnet.resnet18(pretrained=True)
temp_model.fc = torch.nn.Linear(temp_model.fc.in_features, 10)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
temp_model.to(device)

# è‡ªåŠ¨ç¡®å®šæœ€ä¼˜batch size
batch_size = find_optimal_batch_size(temp_model, device, train_set)

# æ¸…ç†ä¸´æ—¶æ¨¡å‹
del temp_model
if torch.cuda.is_available():
    torch.cuda.empty_cache()

def create_optimized_dataloader(dataset, batch_size, is_training=True, num_workers=None):
    """
    åˆ›å»ºé«˜åº¦ä¼˜åŒ–çš„DataLoader
    """
    # è‡ªåŠ¨è®¡ç®—æœ€ä¼˜workeræ•°é‡
    if num_workers is None:
        cpu_count = os.cpu_count() or 4
        
        # æ ¹æ®batch sizeå’Œæ•°æ®é›†å¤§å°è°ƒæ•´workeræ•°
        if batch_size <= 64:
            num_workers = min(4, cpu_count)
        elif batch_size <= 256:
            num_workers = min(6, cpu_count)
        elif batch_size <= 512:
            num_workers = min(8, cpu_count)
        else:  # batch_size > 512
            num_workers = min(4, cpu_count)  # å¤§batch sizeæ—¶å‡å°‘workeré¿å…å†…å­˜ç«äº‰
    
    # æ ¹æ®batch sizeè°ƒæ•´prefetch factor
    if batch_size <= 128:
        prefetch_factor = 4  # å°batchéœ€è¦æ›´å¤šé¢„å–
    elif batch_size <= 512:
        prefetch_factor = 3
    else:
        prefetch_factor = 2  # å¤§batchå‡å°‘é¢„å–é¿å…å†…å­˜å‹åŠ›
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    cuda_available = torch.cuda.is_available()
    
    # é…ç½®å‚æ•°
    loader_config = {
        'batch_size': batch_size,
        'shuffle': is_training,
        'num_workers': num_workers,
        'pin_memory': cuda_available,  # CUDAå¯ç”¨æ—¶å¯ç”¨
        'drop_last': is_training,  # è®­ç»ƒæ—¶ä¸¢å¼ƒä¸å®Œæ•´batch
        'persistent_workers': num_workers > 0,  # æŒä¹…åŒ–workerè¿›ç¨‹
        'prefetch_factor': prefetch_factor if num_workers > 0 else None,
        'generator': torch.Generator().manual_seed(42) if is_training else None,  # å›ºå®šéšæœºç§å­
    }
    
    # é«˜çº§ä¼˜åŒ–é€‰é¡¹
    if cuda_available:
        # ä½¿ç”¨non_blockingä¼ è¾“åŠ é€Ÿæ•°æ®ç§»åŠ¨
        loader_config['pin_memory_device'] = 'cuda'
    
    # å¯¹äºå¤§æ•°æ®é›†ï¼Œé€‚å½“å¢åŠ timeout
    if len(dataset) > 10000:
        loader_config['timeout'] = 60  # 60ç§’è¶…æ—¶
    
    return torch.utils.data.DataLoader(dataset, **loader_config)

def setup_dataloader_optimization():
    """
    è®¾ç½®æ•°æ®åŠ è½½å™¨çš„å…¨å±€ä¼˜åŒ–
    """
    # è®¾ç½®å¤šè¿›ç¨‹æ•°æ®åŠ è½½çš„å…±äº«ç­–ç•¥
    if hasattr(torch.multiprocessing, 'set_sharing_strategy'):
        torch.multiprocessing.set_sharing_strategy('file_system')
    
    # ä¼˜åŒ–OpenMPè®¾ç½®
    if 'OMP_NUM_THREADS' not in os.environ:
        os.environ['OMP_NUM_THREADS'] = '1'
    
    # ä¼˜åŒ–MKLè®¾ç½®
    if 'MKL_NUM_THREADS' not in os.environ:
        os.environ['MKL_NUM_THREADS'] = '1'
    
    print("âš™ï¸  æ•°æ®åŠ è½½å™¨å…¨å±€ä¼˜åŒ–å·²å¯ç”¨")

# åº”ç”¨å…¨å±€ä¼˜åŒ–
setup_dataloader_optimization()

# ä¼˜åŒ–æ•°æ®åŠ è½½å™¨é…ç½®
num_workers = min(8, os.cpu_count() or 4)
if batch_size >= 512:
    num_workers = min(num_workers, 4)

print(f"ğŸ“¦ æ•°æ®åŠ è½½å™¨é…ç½®: batch_size={batch_size}, num_workers={num_workers}")
print(f"ğŸ’» CPUæ ¸å¿ƒæ•°: {os.cpu_count()}, CUDAå¯ç”¨: {torch.cuda.is_available()}")

# åˆ›å»ºä¼˜åŒ–çš„dataloaders
train_loader = create_optimized_dataloader(
    train_set, 
    batch_size=batch_size, 
    is_training=True,
    num_workers=num_workers
)

test_loader = create_optimized_dataloader(
    test_set, 
    batch_size=batch_size, 
    is_training=False,
    num_workers=num_workers
)

# æ˜¾ç¤ºæœ€ç»ˆé…ç½®
print(f"ğŸš€ è®­ç»ƒæ•°æ®åŠ è½½å™¨: batch_size={train_loader.batch_size}, num_workers={train_loader.num_workers}, prefetch_factor={train_loader.prefetch_factor}")
print(f"ğŸ“‹ æµ‹è¯•æ•°æ®åŠ è½½å™¨: batch_size={test_loader.batch_size}, num_workers={test_loader.num_workers}, prefetch_factor={test_loader.prefetch_factor}")
print(f"ğŸ’¾ é«˜çº§é€‰é¡¹: pin_memory={train_loader.pin_memory}, persistent_workers={train_loader.persistent_workers}")

# build model

model = resnet.resnet18(pretrained=True)
model.fc = torch.nn.Linear(model.fc.in_features, 10)  # CIFAR10 has 10 classes

# train the ANN model and save it, validate its accuracy

def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5):
    """
    å¸¦é¢„çƒ­çš„ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))
    
    return LambdaLR(optimizer, lr_lambda)

def save_checkpoint(state, checkpoint_dir, filename, is_best=False):
    """
    ä¿å­˜è®­ç»ƒcheckpoint
    """
    os.makedirs(checkpoint_dir, exist_ok=True)
    filepath = os.path.join(checkpoint_dir, filename)
    torch.save(state, filepath)
    
    if is_best:
        best_filepath = os.path.join(checkpoint_dir, 'best_' + filename)
        torch.save(state, best_filepath)
    
    print(f"ğŸ’¾ Checkpointå·²ä¿å­˜: {filepath}")

def load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None, scaler=None):
    """
    åŠ è½½checkpointæ¢å¤è®­ç»ƒçŠ¶æ€
    """
    if not os.path.isfile(checkpoint_path):
        print(f"âš ï¸  Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return 0, 0.0, False
    
    print(f"ğŸ“‹ æ­£åœ¨åŠ è½½checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
        return 0, 0.0, True  # æ—§æ ¼å¼æ–‡ä»¶ï¼Œåªæœ‰æ¨¡å‹çŠ¶æ€
    
    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
    if scheduler is not None and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    # åŠ è½½æ··åˆç²¾åº¦çŠ¶æ€
    if scaler is not None and 'scaler_state_dict' in checkpoint:
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
    epoch = checkpoint.get('epoch', 0)
    best_acc = checkpoint.get('best_acc', 0.0)
    
    print(f"âœ… æ¢å¤è®­ç»ƒ: epoch={epoch}, best_acc={best_acc:.2f}%")
    return epoch, best_acc, True

def find_best_checkpoint(checkpoint_dir, model_name):
    """
    æŸ¥æ‰¾æœ€ä½³çš„checkpointæ–‡ä»¶ï¼ˆä¼˜å…ˆçº§ï¼šbest > latest > æœ€æ–°epochï¼‰
    """
    if not os.path.exists(checkpoint_dir):
        return None, "best"
    
    # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæŸ¥æ‰¾ best æ–‡ä»¶
    best_file = os.path.join(checkpoint_dir, f"best_{model_name}_checkpoint_*.pth")
    best_files = glob.glob(best_file)
    if best_files:
        # å¦‚æœæœ‰å¤šä¸ªbestæ–‡ä»¶ï¼Œé€‰æ‹©æœ€æ–°çš„
        latest_best = max(best_files, key=os.path.getmtime)
        return latest_best, "best"
    
    # ç¬¬äºŒä¼˜å…ˆçº§ï¼šæŸ¥æ‰¾ best_{model_name}.pth æ–‡ä»¶
    simple_best = os.path.join(checkpoint_dir, f"best_{model_name}.pth")
    if os.path.exists(simple_best):
        return simple_best, "best"
    
    # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šæŸ¥æ‰¾ latest æ–‡ä»¶
    latest_file = os.path.join(checkpoint_dir, f"{model_name}_latest.pth")
    if os.path.exists(latest_file):
        return latest_file, "latest"
    
    # ç¬¬å››ä¼˜å…ˆçº§ï¼šæŸ¥æ‰¾å¸¦epochçš„checkpointæ–‡ä»¶
    pattern = os.path.join(checkpoint_dir, f"{model_name}_checkpoint_*.pth")
    checkpoint_files = glob.glob(pattern)
    
    if checkpoint_files:
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
        latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)
        return latest_checkpoint, "epoch"
    
    # æœ€åå°è¯•ï¼šæŸ¥æ‰¾ä»»ä½•åŒ…å«model_nameçš„.pthæ–‡ä»¶
    pattern = os.path.join(checkpoint_dir, f"*{model_name}*.pth")
    checkpoint_files = glob.glob(pattern)
    
    if checkpoint_files:
        latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)
        return latest_checkpoint, "fallback"
    
    return None, "none"

def create_checkpoint_dir(model_name):
    """
    åˆ›å»º checkpoint ç›®å½•
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_dir = f"checkpoints/{model_name}_{timestamp}"
    os.makedirs(checkpoint_dir, exist_ok=True)
    return checkpoint_dir

def train_model(model, train_loader, test_loader, num_epochs=10, lr=5e-4, model_name="ANN", use_amp=True, resume=True):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ ‡ç­¾å¹³æ»‘
    
    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œweight_decayæå‡æ³›åŒ–èƒ½åŠ›
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    
    # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°å’Œé¢„çƒ­æ­¥æ•°
    num_training_steps = num_epochs * len(train_loader)
    num_warmup_steps = int(0.1 * num_training_steps)  # 10%çš„æ­¥æ•°ç”¨äºé¢„çƒ­
    
    # å¸¦é¢„çƒ­çš„ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = get_cosine_schedule_with_warmup(
        optimizer, num_warmup_steps, num_training_steps
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler('cuda') if use_amp and torch.cuda.is_available() else None
    
    # åˆ›å»º checkpoint ç›®å½•
    checkpoint_dir = create_checkpoint_dir(model_name)
    
    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    best_acc = 0.0
    
    # å°è¯•æ¢å¤è®­ç»ƒ
    if resume:
        print(f"ğŸ” æŸ¥æ‰¾æœ€ä½³checkpointæ–‡ä»¶: {model_name}")
        
        # ä¼˜å…ˆæŸ¥æ‰¾æœ€ä½³æ¨¡å‹
        checkpoint_path, checkpoint_type = find_best_checkpoint("checkpoints", model_name)
        
        if checkpoint_path:
            type_emoji = {
                "best": "ğŸ†",
                "latest": "ğŸ”„", 
                "epoch": "ğŸ“…",
                "fallback": "ğŸ”"
            }
            print(f"{type_emoji.get(checkpoint_type, 'ğŸ“‹')} æ‰¾åˆ°{checkpoint_type}checkpoint: {checkpoint_path}")
            
            start_epoch, best_acc, loaded = load_checkpoint(
                checkpoint_path, model, optimizer, scheduler, scaler
            )
            if loaded:
                if checkpoint_type == "best":
                    # ä»æœ€ä½³æ¨¡å‹å¼€å§‹ï¼Œé‡æ–°è®­ç»ƒä»¥è¿›ä¸€æ­¥ä¼˜åŒ–
                    print(f"ğŸ† ä»æœ€ä½³æ¨¡å‹å¼€å§‹ï¼ˆbest_acc={best_acc:.2f}%ï¼‰ï¼Œç»§ç»­ä¼˜åŒ–")
                    # å¯ä»¥é€‰æ‹©ä»æœ€ä½³epochç»§ç»­ï¼Œæˆ–è€…é‡æ–°å¼€å§‹ä½†ä¿æŒbest_acc
                    start_epoch = max(0, start_epoch)  # ä»æœ€ä½³epochç»§ç»­
                else:
                    start_epoch += 1  # ä»ä¸‹ä¸€ä¸ªepochå¼€å§‹
                    print(f"ğŸ”„ æ¢å¤è®­ç»ƒï¼Œä» epoch {start_epoch} å¼€å§‹")
        else:
            print(f"ğŸ† æœªæ‰¾åˆ°ä»»ä½•checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
            print(f"    - æŸ¥æ‰¾è·¯å¾„: checkpoints/")
            print(f"    - checkpointsç›®å½•å­˜åœ¨: {os.path.exists('checkpoints')}")
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ{model_name}æ¨¡å‹...")
    print(f"ğŸ“Š è®¾å¤‡: {device}, æ··åˆç²¾åº¦: {use_amp and torch.cuda.is_available()}, Batch Size: {train_loader.batch_size}")
    print(f"ğŸ“ˆ æ€»æ­¥æ•°: {num_training_steps}, é¢„çƒ­æ­¥æ•°: {num_warmup_steps}")
    print(f"ğŸ’¾ Checkpointç›®å½•: {checkpoint_dir}")
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºepochè¿›åº¦
    epoch_pbar = tqdm(range(start_epoch, num_epochs), desc=f"è®­ç»ƒ{model_name}", unit="epoch")
    
    for epoch in epoch_pbar:
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        start_time = time.time()
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºbatchè¿›åº¦
        batch_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}", 
                         leave=False, unit="batch")
        
        for batch_idx, (data, target) in enumerate(batch_pbar):
            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            if scaler is not None:
                with autocast('cuda'):
                    output = model(data)
                    loss = criterion(output, target)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
            
            scheduler.step()  # æ¯ä¸ªbatchåæ›´æ–°å­¦ä¹ ç‡
            
            running_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            # æ›´æ–°batchè¿›åº¦æ¡
            current_acc = 100. * correct / total
            current_lr = optimizer.param_groups[0]['lr']
            batch_pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_acc:.2f}%',
                'LR': f'{current_lr:.2e}'
            })
        
        epoch_time = time.time() - start_time
        train_acc = 100. * correct / total
        test_acc = validate_model(model, test_loader)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        is_best = test_acc > best_acc
        if is_best:
            best_acc = test_acc
        
        # ä¿å­˜checkpoint
        checkpoint_state = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_acc': best_acc,
            'train_acc': train_acc,
            'test_acc': test_acc,
            'loss': running_loss / len(train_loader),
        }
        
        if scaler is not None:
            checkpoint_state['scaler_state_dict'] = scaler.state_dict()
        
        # æ¯10ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochä¿å­˜checkpoint
        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1 or is_best:
            save_checkpoint(
                checkpoint_state, 
                checkpoint_dir, 
                f"{model_name}_checkpoint_epoch_{epoch:03d}.pth",
                is_best=is_best
            )
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¹Ÿä¿å­˜ä¸€ä¸ªç®€å•åç§°çš„bestæ–‡ä»¶
        if is_best:
            torch.save(model.state_dict(), f'best_{model_name}.pth')
            print(f"ğŸ† æœ€ä½³æ¨¡å‹å·²æ›´æ–°: best_{model_name}.pth (acc={best_acc:.2f}%)")
        
        # ä¿å­˜æœ€æ–°çš„checkpointï¼ˆç”¨äºresumeï¼‰
        os.makedirs("checkpoints", exist_ok=True)
        save_checkpoint(
            checkpoint_state,
            "checkpoints", 
            f"{model_name}_latest.pth"
        )
        
        # æ›´æ–°epochè¿›åº¦æ¡
        epoch_pbar.set_postfix({
            'Train Acc': f'{train_acc:.2f}%',
            'Test Acc': f'{test_acc:.2f}%',
            'Best': f'{best_acc:.2f}%',
            'Time': f'{epoch_time:.1f}s'
        })
    
    print(f"âœ… {model_name}è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%")
    return model

def validate_model(model, test_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        # ä½¿ç”¨tqdmæ˜¾ç¤ºéªŒè¯è¿›åº¦
        val_pbar = tqdm(test_loader, desc="éªŒè¯ä¸­", leave=False, unit="batch")
        
        for data, target in val_pbar:
            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
            
            # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
            if torch.cuda.is_available():
                with autocast('cuda'):
                    output = model(data)
            else:
                output = model(data)
            
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            # æ›´æ–°éªŒè¯è¿›åº¦æ¡
            current_acc = 100. * correct / total
            val_pbar.set_postfix({'Acc': f'{current_acc:.2f}%'})
    
    accuracy = 100. * correct / total
    return accuracy

# è®­ç»ƒANNæ¨¡å‹
print("=== ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒANNæ¨¡å‹ ===")
# é•¿æ—¶é—´è®­ç»ƒä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼Œè‡ªåŠ¨ä»æœ€ä½³checkpointæ¢å¤
model = train_model(model, train_loader, test_loader, num_epochs=200, lr=0.01, model_name="ANN", resume=True)
torch.save(model.state_dict(), 'ANN.pth')
print("ANNæ¨¡å‹å·²ä¿å­˜ä¸º ANN.pth")

# ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒç‹¬ç«‹çš„QANNæ¨¡å‹
print("\n=== ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒQANNæ¨¡å‹ ===")

# åˆ›å»ºæ–°çš„QANNæ¨¡å‹ï¼ˆä»ANNæ¨¡å‹åˆå§‹åŒ–ï¼‰
print("åˆ›å»ºQANNæ¨¡å‹...")
qann_model = resnet.resnet18(pretrained=False)
qann_model.fc = torch.nn.Linear(qann_model.fc.in_features, 10)

# åŠ è½½è®­ç»ƒå¥½çš„ANNæƒé‡ä½œä¸ºQANNçš„åˆå§‹åŒ–
print("ä½¿ç”¨è®­ç»ƒå¥½çš„ANNæƒé‡åˆå§‹åŒ–QANNæ¨¡å‹...")
qann_model.load_state_dict(model.state_dict())

# è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
print("åº”ç”¨é‡åŒ–æ“ä½œ...")
myquan_replace_resnet(qann_model, level=8, weight_bit=32, is_softmax=False)
print("æ¨¡å‹å·²è½¬æ¢ä¸ºQANNç»“æ„")

# éªŒè¯è½¬æ¢åçš„QANNæ¨¡å‹å‡†ç¡®ç‡
qann_acc_initial = validate_model(qann_model, test_loader)
print(f"åˆå§‹QANNæ¨¡å‹éªŒè¯å‡†ç¡®ç‡: {qann_acc_initial:.2f}%")

# è®­ç»ƒQANNæ¨¡å‹
print("\nå¼€å§‹è®­ç»ƒQANNæ¨¡å‹...")
# QANNéœ€è¦é‡æ–°è®­ç»ƒä»¥é€‚åº”é‡åŒ–æ“ä½œï¼Œä»å¤´è®­ç»ƒè·å¾—æœ€ä½³é‡åŒ–æ€§èƒ½
qann_model = train_model(qann_model, train_loader, test_loader, num_epochs=100, lr=0.001, model_name="QANN", resume=True)

# ä¿å­˜è®­ç»ƒå¥½çš„QANNæ¨¡å‹
torch.save(qann_model.state_dict(), 'QANN.pth')
# åŒæ—¶ä¿å­˜æœ€ä½³QANNæ¨¡å‹
torch.save(qann_model.state_dict(), 'best_QANN.pth')
print("QANNæ¨¡å‹å·²ä¿å­˜ä¸º QANN.pth å’Œ best_QANN.pth")

# convert QANN model to SNN model, validate its accuracy
print("\n=== ç¬¬ä¸‰é˜¶æ®µï¼šè½¬æ¢ä¸ºSNNæ¨¡å‹ ===")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
output_dir = "/home/zilingwei/output_bin_snn_resnet_w32_a4_T8/"
os.makedirs(output_dir, exist_ok=True)

# ä½¿ç”¨è®­ç»ƒå¥½çš„QANNæ¨¡å‹åˆ›å»ºSNN
print("ä½¿ç”¨è®­ç»ƒå¥½çš„QANNæ¨¡å‹åˆ›å»ºSNN...")
snn_model = SNNWrapper_MS(ann_model=qann_model, cfg=None, time_step=8, \
                    Encoding_type="analog", level=8, neuron_type="ST-BIF", \
                    model_name="resnet", is_softmax = False,  suppress_over_fire = False,\
                    record_inout=False,learnable=True,record_dir=output_dir)
print("æ¨¡å‹å·²è½¬æ¢ä¸ºSNN")

def train_snn_model(snn_model, train_loader, test_loader, num_epochs=100, lr=0.001, resume=True):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    snn_model.to(device)
    
    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)  # è½»å¾®æ ‡ç­¾å¹³æ»‘
    optimizer = optim.AdamW(snn_model.parameters(), lr=lr, weight_decay=5e-5)
    
    # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°å’Œé¢„çƒ­æ­¥æ•°
    num_training_steps = num_epochs * len(train_loader)
    num_warmup_steps = int(0.05 * num_training_steps)  # 5%çš„æ­¥æ•°ç”¨äºé¢„çƒ­ï¼ˆSNNè¾ƒæ•æ„Ÿï¼‰
    
    # å¸¦é¢„çƒ­çš„ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = get_cosine_schedule_with_warmup(
        optimizer, num_warmup_steps, num_training_steps
    )
    
    # åˆ›å»º checkpoint ç›®å½•
    checkpoint_dir = create_checkpoint_dir("SNN")
    
    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    best_acc = 0.0
    
    # å°è¯•æ¢å¤è®­ç»ƒ
    if resume:
        print(f"ğŸ” æŸ¥æ‰¾æœ€ä½³SNN checkpointæ–‡ä»¶")
        
        checkpoint_path, checkpoint_type = find_best_checkpoint("checkpoints", "SNN")
        
        if checkpoint_path:
            type_emoji = {
                "best": "ğŸ†",
                "latest": "ğŸ”„", 
                "epoch": "ğŸ“…",
                "fallback": "ğŸ”"
            }
            print(f"{type_emoji.get(checkpoint_type, 'ğŸ“‹')} æ‰¾åˆ°SNN {checkpoint_type}checkpoint: {checkpoint_path}")
            
            start_epoch, best_acc, loaded = load_checkpoint(
                checkpoint_path, snn_model, optimizer, scheduler
            )
            if loaded:
                if checkpoint_type == "best":
                    print(f"ğŸ† ä»æœ€ä½³SNNæ¨¡å‹å¼€å§‹ï¼ˆbest_acc={best_acc:.2f}%ï¼‰ï¼Œç»§ç»­ä¼˜åŒ–")
                    start_epoch = max(0, start_epoch)
                else:
                    start_epoch += 1
                    print(f"ğŸ”„ æ¢å¤SNNè®­ç»ƒï¼Œä» epoch {start_epoch} å¼€å§‹")
        else:
            print(f"ğŸ† æœªæ‰¾åˆ°SNN checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
            print(f"    - æŸ¥æ‰¾è·¯å¾„: checkpoints/")
            print(f"    - checkpointsç›®å½•å­˜åœ¨: {os.path.exists('checkpoints')}")
    
    print(f"ğŸ§  å¼€å§‹è®­ç»ƒSNNæ¨¡å‹...")
    print(f"ğŸ“Š æ€»æ­¥æ•°: {num_training_steps}, é¢„çƒ­æ­¥æ•°: {num_warmup_steps}")
    print(f"ğŸ’¾ Checkpointç›®å½•: {checkpoint_dir}")
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºepochè¿›åº¦
    epoch_pbar = tqdm(range(start_epoch, num_epochs), desc="è®­ç»ƒSNN", unit="epoch")
    
    for epoch in epoch_pbar:
        snn_model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        start_time = time.time()
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºbatchè¿›åº¦
        batch_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}", 
                         leave=False, unit="batch")
        
        for batch_idx, (data, target) in enumerate(batch_pbar):
            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            # SNNæ¨¡å‹é€šå¸¸ä¸é€‚åˆæ··åˆç²¾åº¦ï¼Œå› ä¸ºæ¶‰åŠå¤æ‚çš„è„‰å†²æ“ä½œ
            output = snn_model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(snn_model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()  # æ¯ä¸ªbatchåæ›´æ–°å­¦ä¹ ç‡
            
            running_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            # æ›´æ–°batchè¿›åº¦æ¡
            current_acc = 100. * correct / total
            current_lr = optimizer.param_groups[0]['lr']
            batch_pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_acc:.2f}%',
                'LR': f'{current_lr:.2e}'
            })
        
        epoch_time = time.time() - start_time
        train_acc = 100. * correct / total
        test_acc = validate_snn_model(snn_model, test_loader)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        is_best = test_acc > best_acc
        if is_best:
            best_acc = test_acc
        
        # ä¿å­˜checkpoint
        checkpoint_state = {
            'epoch': epoch,
            'model_state_dict': snn_model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_acc': best_acc,
            'train_acc': train_acc,
            'test_acc': test_acc,
            'loss': running_loss / len(train_loader),
        }
        
        # æ¯15ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochä¿å­˜checkpoint
        if (epoch + 1) % 15 == 0 or epoch == num_epochs - 1 or is_best:
            save_checkpoint(
                checkpoint_state, 
                checkpoint_dir, 
                f"SNN_checkpoint_epoch_{epoch:03d}.pth",
                is_best=is_best
            )
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¹Ÿä¿å­˜ä¸€ä¸ªç®€å•åç§°çš„bestæ–‡ä»¶
        if is_best:
            torch.save(snn_model.state_dict(), 'best_SNN.pth')
            print(f"ğŸ† æœ€ä½³SNNæ¨¡å‹å·²æ›´æ–°: best_SNN.pth (acc={best_acc:.2f}%)")
        
        # ä¿å­˜æœ€æ–°çš„checkpoint
        os.makedirs("checkpoints", exist_ok=True)
        save_checkpoint(
            checkpoint_state,
            "checkpoints", 
            "SNN_latest.pth"
        )
        
        # æ›´æ–°epochè¿›åº¦æ¡
        epoch_pbar.set_postfix({
            'Train Acc': f'{train_acc:.2f}%',
            'Test Acc': f'{test_acc:.2f}%',
            'Best': f'{best_acc:.2f}%',
            'Time': f'{epoch_time:.1f}s'
        })
    
    print(f"âœ… SNNè®­ç»ƒå®Œæˆï¼Œæœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%")
    return snn_model

def validate_snn_model(snn_model, test_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    snn_model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = snn_model(data)
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
    
    accuracy = 100. * correct / total
    print(f'SNNéªŒè¯å‡†ç¡®ç‡: {accuracy:.2f}%')
    return accuracy

# éªŒè¯åˆå§‹SNNæ¨¡å‹å‡†ç¡®ç‡
snn_acc_initial = validate_snn_model(snn_model, test_loader)
print(f"åˆå§‹SNNæ¨¡å‹éªŒè¯å‡†ç¡®ç‡: {snn_acc_initial:.2f}%")

# train the SNN model and save it as SNN.pth
print("\nå¼€å§‹è®­ç»ƒSNNæ¨¡å‹...")
# SNNé•¿æ—¶é—´è®­ç»ƒä»¥ä¼˜åŒ–æ—¶åºç‰¹æ€§ï¼Œè‡ªåŠ¨ä»æœ€ä½³checkpointæ¢å¤
snn_model = train_snn_model(snn_model, train_loader, test_loader, num_epochs=100, lr=0.001, resume=True)
torch.save(snn_model.state_dict(), 'SNN.pth')
print("SNNæ¨¡å‹å·²ä¿å­˜ä¸º SNN.pth")

print("\n=== è®­ç»ƒå®Œæˆï¼æ¨¡å‹æ€§èƒ½æ€»ç»“ ===")
print(f"æœ€ç»ˆSNNæ¨¡å‹éªŒè¯å‡†ç¡®ç‡: {validate_snn_model(snn_model, test_loader):.2f}%")