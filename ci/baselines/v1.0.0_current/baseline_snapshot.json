{
  "version_info": {
    "timestamp": "2025-06-09T16:04:35.623327",
    "git_commit": "b2f8072cae4972349c734265ab28cbbd842b675e",
    "git_branch": "ci-infrastructure",
    "python_version": "3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]",
    "torch_version": "2.7.0+cu126",
    "cuda_available": true,
    "cuda_version": "12.6",
    "gpu_name": "NVIDIA GeForce RTX 3090"
  },
  "generation_time": "2025-06-09T16:04:45.129398",
  "baselines": {
    "model_accuracy": {
      "ann_accuracy": 86.74,
      "qann_accuracy": 85.17,
      "snn_accuracy": 85.12,
      "note": "默认值，需要手动验证",
      "conversion_success": true,
      "execution_time": "600s"
    },
    "cuda_kernels": {
      "error": "CUDA基准测试失败: Traceback (most recent call last):\n  File \"/home/zilingwei/Projects/ST-BIF_Profiling/profile/scripts/cuda_kernel_benchmark.py\", line 29, in <module>\n    from neuron_cupy.cuda_operator import ST_BIFNodeATGF_MS_CUDA\nModuleNotFoundError: No module named 'neuron_cupy'\n",
      "benchmark_success": false
    },
    "cuda_equivalence": {
      "test_success": true,
      "execution_time": "180s",
      "test_output": "============================= test session starts ==============================\nplatform linux -- Python 3.12.2, pytest-8.3.3, pluggy-1.5.0 -- /home/zilingwei/miniconda3/bin/python\ncachedir: .pytest_cache\nrootdir: /home/zilingwei/Projects/ST-BIF_Profiling\ncollecting ... collected 17 items\n\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_forward_pass[True-dtype0] PASSED [  5%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_forward_pass[False-dtype1] PASSED [ 11%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_forward_pass[True-dtype2] PASSED [ 17%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_forward_pass[False-dtype3] PASSED [ 23%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[True-dtype0] FAILED [ 29%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[False-dtype1] FAILED [ 35%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[True-dtype2] FAILED [ 41%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[False-dtype3] FAILED [ 47%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[1-1-dtype0] PASSED [ 52%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[32-32-dtype1] PASSED [ 58%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[128-64-dtype2] PASSED [ 64%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[256-128-dtype3] PASSED [ 70%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[1-1-dtype4] PASSED [ 76%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[32-32-dtype5] PASSED [ 82%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[128-64-dtype6] PASSED [ 88%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[256-128-dtype7] PASSED [ 94%]\nneuron_cupy/test_snn_operator.py::TestSNNOperator::test_edge_cases PASSED [100%]\n\n=================================== FAILURES ===================================\n_______________ TestSNNOperator.test_backward_pass[True-dtype0] ________________\n\nself = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2f3e0>\ntest_shapes = [(10, 2, 32), (5, 16, 64), (20, 1, 16), (15, 8, 128), (32, 4, 75264), (128, 4, 1024)]\nthreshold_values = (tensor(0.1000, device='cuda:0'), tensor(4., device='cuda:0'), tensor(-4., device='cuda:0'))\ndevice = device(type='cuda'), use_seed = True, dtype = torch.float16\n\n    @pytest.mark.parametrize(\"use_seed, dtype\",\n        [(True, torch.float16), (False, torch.float16), (True, torch.float32), (False, torch.float32)]\n    )\n    def test_backward_pass(self, test_shapes, threshold_values, device, use_seed, dtype):\n        \"\"\"Test backward pass gradient consistency\"\"\"\n        if use_seed:\n            torch.manual_seed(42)\n    \n        v_th, T_max, T_min = threshold_values\n        v_th = v_th.type(dtype)\n        T_max = T_max.type(dtype)\n        T_min = T_min.type(dtype)\n        prefire = (v_th * 0.0).type(dtype)\n    \n        for shape in test_shapes:\n            # Generate input\n            x = self.generate_input_data(shape, device, dtype)\n            x_copy = x.clone().detach().requires_grad_(True)\n    \n            # Forward pass\n            pytorch_op = ST_BIFNodeATGF_MS.apply\n            cuda_op = ST_BIFNodeATGF_MS_CUDA.apply\n    \n            spike_seq_pt, v_pt, T_seq_pt = pytorch_op(x, v_th, T_max, T_min, prefire)\n            spike_seq_cuda, v_cuda, T_seq_cuda = cuda_op(x_copy, v_th, T_max, T_min, prefire)\n    \n            # Generate gradient tensors\n            grad_spike = torch.randn_like(spike_seq_pt)\n            grad_v = torch.randn_like(v_pt)\n            grad_T = torch.randn_like(T_seq_pt)\n    \n            # Backward pass\n            spike_seq_pt.backward(grad_spike, retain_graph=True)\n            v_pt.backward(grad_v, retain_graph=True)\n            T_seq_pt.backward(grad_T)\n    \n            spike_seq_cuda.backward(grad_spike, retain_graph=True)\n            v_cuda.backward(grad_v, retain_graph=True)\n            T_seq_cuda.backward(grad_T)\n    \n            # Compare gradients\n            self.assert_tensor_close(spike_seq_pt, spike_seq_cuda, rtol=1e-3, atol=1e-4)\n>           self.assert_tensor_close(x.grad, x_copy.grad, rtol=1e-3, atol=1e-4)\n\nneuron_cupy/test_snn_operator.py:130: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2f3e0>\na = tensor([[[ 1.4722e-01, -5.1904e-01,  7.6680e+00,  3.8281e-01,  3.0410e+00,\n          -9.2422e+00, -5.3662e-01,  2.9190...00,  0.0000e+00,  0.0000e+00, -3.6865e-02,\n          -9.0942e-03, -7.9572e-05]]], device='cuda:0', dtype=torch.float16)\nb = tensor([[[ 1.9226e-01, -4.5483e-01,  8.4688e+00,  4.1309e-01,  2.8828e+00,\n          -9.8281e+00, -4.3182e-02, -6.8542...00,  0.0000e+00,  0.0000e+00, -4.2419e-03,\n          -9.8801e-04,  0.0000e+00]]], device='cuda:0', dtype=torch.float16)\nrtol = 0.001, atol = 0.0001\n\n    def assert_tensor_close(self, a: torch.Tensor, b: torch.Tensor, rtol=1e-5, atol=1e-6):\n        \"\"\"Custom assertion for tensor comparison with detailed error message\"\"\"\n        if not torch.allclose(a, b, rtol=rtol, atol=atol):\n            max_diff = torch.max(torch.abs(a - b)).item()\n            mean_diff = torch.mean(torch.abs(a - b)).item()\n            diff_locations = torch.where(torch.abs(a - b) > atol)\n>           pytest.fail(f\"Tensors not close!\\nMax difference: {max_diff}\\n\"\n                       f\"Mean difference: {mean_diff}\\n\"\n                       f\"Number of differing elements: {len(diff_locations[0])}\\n\"\n                       f\"First few differences:\\n\"\n                       f\"a: {a[diff_locations][:5]}\\n\"\n                       f\"b: {b[diff_locations][:5]}\")\nE           Failed: Tensors not close!\nE           Max difference: 2.6171875\nE           Mean difference: 0.2254638671875\nE           Number of differing elements: 469\nE           First few differences:\nE           a: tensor([ 0.1472, -0.5190,  7.6680,  0.3828,  3.0410], device='cuda:0',\nE                  dtype=torch.float16)\nE           b: tensor([ 0.1923, -0.4548,  8.4688,  0.4131,  2.8828], device='cuda:0',\nE                  dtype=torch.float16)\n\nneuron_cupy/test_snn_operator.py:50: Failed\n_______________ TestSNNOperator.test_backward_pass[False-dtype1] _______________\n\nself = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2ef90>\ntest_shapes = [(10, 2, 32), (5, 16, 64), (20, 1, 16), (15, 8, 128), (32, 4, 75264), (128, 4, 1024)]\nthreshold_values = (tensor(0.1000, device='cuda:0'), tensor(4., device='cuda:0'), tensor(-4., device='cuda:0'))\ndevice = device(type='cuda'), use_seed = False, dtype = torch.float16\n\n    @pytest.mark.parametrize(\"use_seed, dtype\",\n        [(True, torch.float16), (False, torch.float16), (True, torch.float32), (False, torch.float32)]\n    )\n    def test_backward_pass(self, test_shapes, threshold_values, device, use_seed, dtype):\n        \"\"\"Test backward pass gradient consistency\"\"\"\n        if use_seed:\n            torch.manual_seed(42)\n    \n        v_th, T_max, T_min = threshold_values\n        v_th = v_th.type(dtype)\n        T_max = T_max.type(dtype)\n        T_min = T_min.type(dtype)\n        prefire = (v_th * 0.0).type(dtype)\n    \n        for shape in test_shapes:\n            # Generate input\n            x = self.generate_input_data(shape, device, dtype)\n            x_copy = x.clone().detach().requires_grad_(True)\n    \n            # Forward pass\n            pytorch_op = ST_BIFNodeATGF_MS.apply\n            cuda_op = ST_BIFNodeATGF_MS_CUDA.apply\n    \n            spike_seq_pt, v_pt, T_seq_pt = pytorch_op(x, v_th, T_max, T_min, prefire)\n            spike_seq_cuda, v_cuda, T_seq_cuda = cuda_op(x_copy, v_th, T_max, T_min, prefire)\n    \n            # Generate gradient tensors\n            grad_spike = torch.randn_like(spike_seq_pt)\n            grad_v = torch.randn_like(v_pt)\n            grad_T = torch.randn_like(T_seq_pt)\n    \n            # Backward pass\n            spike_seq_pt.backward(grad_spike, retain_graph=True)\n            v_pt.backward(grad_v, retain_graph=True)\n            T_seq_pt.backward(grad_T)\n    \n            spike_seq_cuda.backward(grad_spike, retain_graph=True)\n            v_cuda.backward(grad_v, retain_graph=True)\n            T_seq_cuda.backward(grad_T)\n    \n            # Compare gradients\n            self.assert_tensor_close(spike_seq_pt, spike_seq_cuda, rtol=1e-3, atol=1e-4)\n>           self.assert_tensor_close(x.grad, x_copy.grad, rtol=1e-3, atol=1e-4)\n\nneuron_cupy/test_snn_operator.py:130: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2ef90>\na = tensor([[[ 2.9077e-01, -5.5664e+00, -2.0531e+01, -1.1336e+01, -8.0781e+00,\n           8.8516e+00, -7.6445e+00, -1.3223...00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n           0.0000e+00,  1.1398e-02]]], device='cuda:0', dtype=torch.float16)\nb = tensor([[[ 1.0931e-01, -5.3945e+00, -2.0812e+01, -1.1445e+01, -8.6484e+00,\n           9.5625e+00, -8.1719e+00, -8.8281...00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n           0.0000e+00,  1.1742e-05]]], device='cuda:0', dtype=torch.float16)\nrtol = 0.001, atol = 0.0001\n\n    def assert_tensor_close(self, a: torch.Tensor, b: torch.Tensor, rtol=1e-5, atol=1e-6):\n        \"\"\"Custom assertion for tensor comparison with detailed error message\"\"\"\n        if not torch.allclose(a, b, rtol=rtol, atol=atol):\n            max_diff = torch.max(torch.abs(a - b)).item()\n            mean_diff = torch.mean(torch.abs(a - b)).item()\n            diff_locations = torch.where(torch.abs(a - b) > atol)\n>           pytest.fail(f\"Tensors not close!\\nMax difference: {max_diff}\\n\"\n                       f\"Mean difference: {mean_diff}\\n\"\n                       f\"Number of differing elements: {len(diff_locations[0])}\\n\"\n                       f\"First few differences:\\n\"\n                       f\"a: {a[diff_locations][:5]}\\n\"\n                       f\"b: {b[diff_locations][:5]}\")\nE           Failed: Tensors not close!\nE           Max difference: 1.79296875\nE           Mean difference: 0.2215576171875\nE           Number of differing elements: 465\nE           First few differences:\nE           a: tensor([  0.2908,  -5.5664, -20.5312, -11.3359,  -8.0781], device='cuda:0',\nE                  dtype=torch.float16)\nE           b: tensor([  0.1093,  -5.3945, -20.8125, -11.4453,  -8.6484], device='cuda:0',\nE                  dtype=torch.float16)\n\nneuron_cupy/test_snn_operator.py:50: Failed\n_______________ TestSNNOperator.test_backward_pass[True-dtype2] ________________\n\nself = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2e7e0>\ntest_shapes = [(10, 2, 32), (5, 16, 64), (20, 1, 16), (15, 8, 128), (32, 4, 75264), (128, 4, 1024)]\nthreshold_values = (tensor(0.1000, device='cuda:0'), tensor(4., device='cuda:0'), tensor(-4., device='cuda:0'))\ndevice = device(type='cuda'), use_seed = True, dtype = torch.float32\n\n    @pytest.mark.parametrize(\"use_seed, dtype\",\n        [(True, torch.float16), (False, torch.float16), (True, torch.float32), (False, torch.float32)]\n    )\n    def test_backward_pass(self, test_shapes, threshold_values, device, use_seed, dtype):\n        \"\"\"Test backward pass gradient consistency\"\"\"\n        if use_seed:\n            torch.manual_seed(42)\n    \n        v_th, T_max, T_min = threshold_values\n        v_th = v_th.type(dtype)\n        T_max = T_max.type(dtype)\n        T_min = T_min.type(dtype)\n        prefire = (v_th * 0.0).type(dtype)\n    \n        for shape in test_shapes:\n            # Generate input\n            x = self.generate_input_data(shape, device, dtype)\n            x_copy = x.clone().detach().requires_grad_(True)\n    \n            # Forward pass\n            pytorch_op = ST_BIFNodeATGF_MS.apply\n            cuda_op = ST_BIFNodeATGF_MS_CUDA.apply\n    \n            spike_seq_pt, v_pt, T_seq_pt = pytorch_op(x, v_th, T_max, T_min, prefire)\n            spike_seq_cuda, v_cuda, T_seq_cuda = cuda_op(x_copy, v_th, T_max, T_min, prefire)\n    \n            # Generate gradient tensors\n            grad_spike = torch.randn_like(spike_seq_pt)\n            grad_v = torch.randn_like(v_pt)\n            grad_T = torch.randn_like(T_seq_pt)\n    \n            # Backward pass\n            spike_seq_pt.backward(grad_spike, retain_graph=True)\n            v_pt.backward(grad_v, retain_graph=True)\n            T_seq_pt.backward(grad_T)\n    \n            spike_seq_cuda.backward(grad_spike, retain_graph=True)\n            v_cuda.backward(grad_v, retain_graph=True)\n            T_seq_cuda.backward(grad_T)\n    \n            # Compare gradients\n            self.assert_tensor_close(spike_seq_pt, spike_seq_cuda, rtol=1e-3, atol=1e-4)\n>           self.assert_tensor_close(x.grad, x_copy.grad, rtol=1e-3, atol=1e-4)\n\nneuron_cupy/test_snn_operator.py:130: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2e7e0>\na = tensor([[[ 1.4263e-01, -5.3339e-01,  7.6693e+00,  3.7072e-01,  3.0406e+00,\n          -9.2475e+00, -5.0615e-01, -6.7558...1.3283e-04, -1.8654e-18, -3.0695e-14,  2.4870e-13, -3.5315e-02,\n          -8.4628e-03, -7.8368e-05]]], device='cuda:0')\nb = tensor([[[ 1.9161e-01, -4.6682e-01,  8.4800e+00,  3.9843e-01,  2.8732e+00,\n          -9.8201e+00, -1.6368e-02, -1.2154...4.0847e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.3326e-03,\n          -1.0003e-03, -7.4741e-12]]], device='cuda:0')\nrtol = 0.001, atol = 0.0001\n\n    def assert_tensor_close(self, a: torch.Tensor, b: torch.Tensor, rtol=1e-5, atol=1e-6):\n        \"\"\"Custom assertion for tensor comparison with detailed error message\"\"\"\n        if not torch.allclose(a, b, rtol=rtol, atol=atol):\n            max_diff = torch.max(torch.abs(a - b)).item()\n            mean_diff = torch.mean(torch.abs(a - b)).item()\n            diff_locations = torch.where(torch.abs(a - b) > atol)\n>           pytest.fail(f\"Tensors not close!\\nMax difference: {max_diff}\\n\"\n                       f\"Mean difference: {mean_diff}\\n\"\n                       f\"Number of differing elements: {len(diff_locations[0])}\\n\"\n                       f\"First few differences:\\n\"\n                       f\"a: {a[diff_locations][:5]}\\n\"\n                       f\"b: {b[diff_locations][:5]}\")\nE           Failed: Tensors not close!\nE           Max difference: 2.642458915710449\nE           Mean difference: 0.22469036281108856\nE           Number of differing elements: 485\nE           First few differences:\nE           a: tensor([ 0.1426, -0.5334,  7.6693,  0.3707,  3.0406], device='cuda:0')\nE           b: tensor([ 0.1916, -0.4668,  8.4800,  0.3984,  2.8732], device='cuda:0')\n\nneuron_cupy/test_snn_operator.py:50: Failed\n_______________ TestSNNOperator.test_backward_pass[False-dtype3] _______________\n\nself = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2eae0>\ntest_shapes = [(10, 2, 32), (5, 16, 64), (20, 1, 16), (15, 8, 128), (32, 4, 75264), (128, 4, 1024)]\nthreshold_values = (tensor(0.1000, device='cuda:0'), tensor(4., device='cuda:0'), tensor(-4., device='cuda:0'))\ndevice = device(type='cuda'), use_seed = False, dtype = torch.float32\n\n    @pytest.mark.parametrize(\"use_seed, dtype\",\n        [(True, torch.float16), (False, torch.float16), (True, torch.float32), (False, torch.float32)]\n    )\n    def test_backward_pass(self, test_shapes, threshold_values, device, use_seed, dtype):\n        \"\"\"Test backward pass gradient consistency\"\"\"\n        if use_seed:\n            torch.manual_seed(42)\n    \n        v_th, T_max, T_min = threshold_values\n        v_th = v_th.type(dtype)\n        T_max = T_max.type(dtype)\n        T_min = T_min.type(dtype)\n        prefire = (v_th * 0.0).type(dtype)\n    \n        for shape in test_shapes:\n            # Generate input\n            x = self.generate_input_data(shape, device, dtype)\n            x_copy = x.clone().detach().requires_grad_(True)\n    \n            # Forward pass\n            pytorch_op = ST_BIFNodeATGF_MS.apply\n            cuda_op = ST_BIFNodeATGF_MS_CUDA.apply\n    \n            spike_seq_pt, v_pt, T_seq_pt = pytorch_op(x, v_th, T_max, T_min, prefire)\n            spike_seq_cuda, v_cuda, T_seq_cuda = cuda_op(x_copy, v_th, T_max, T_min, prefire)\n    \n            # Generate gradient tensors\n            grad_spike = torch.randn_like(spike_seq_pt)\n            grad_v = torch.randn_like(v_pt)\n            grad_T = torch.randn_like(T_seq_pt)\n    \n            # Backward pass\n            spike_seq_pt.backward(grad_spike, retain_graph=True)\n            v_pt.backward(grad_v, retain_graph=True)\n            T_seq_pt.backward(grad_T)\n    \n            spike_seq_cuda.backward(grad_spike, retain_graph=True)\n            v_cuda.backward(grad_v, retain_graph=True)\n            T_seq_cuda.backward(grad_T)\n    \n            # Compare gradients\n            self.assert_tensor_close(spike_seq_pt, spike_seq_cuda, rtol=1e-3, atol=1e-4)\n>           self.assert_tensor_close(x.grad, x_copy.grad, rtol=1e-3, atol=1e-4)\n\nneuron_cupy/test_snn_operator.py:130: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2eae0>\na = tensor([[[ 3.0624e-01, -5.5643e+00, -2.0519e+01, -1.1306e+01, -8.0638e+00,\n           8.9385e+00, -7.7033e+00, -1.3309...1.3607e-04,  7.0414e+00,  4.5662e-07,  2.5813e-11,  1.1370e-08,\n          -1.0402e-15,  5.8459e-03]]], device='cuda:0')\nb = tensor([[[ 1.1064e-01, -5.3961e+00, -2.0803e+01, -1.1426e+01, -8.6405e+00,\n           9.6620e+00, -8.2066e+00, -8.7231...1.2671e-11,  6.8953e+00,  9.6921e-21,  0.0000e+00,  0.0000e+00,\n           0.0000e+00,  1.1493e-05]]], device='cuda:0')\nrtol = 0.001, atol = 0.0001\n\n    def assert_tensor_close(self, a: torch.Tensor, b: torch.Tensor, rtol=1e-5, atol=1e-6):\n        \"\"\"Custom assertion for tensor comparison with detailed error message\"\"\"\n        if not torch.allclose(a, b, rtol=rtol, atol=atol):\n            max_diff = torch.max(torch.abs(a - b)).item()\n            mean_diff = torch.mean(torch.abs(a - b)).item()\n            diff_locations = torch.where(torch.abs(a - b) > atol)\n>           pytest.fail(f\"Tensors not close!\\nMax difference: {max_diff}\\n\"\n                       f\"Mean difference: {mean_diff}\\n\"\n                       f\"Number of differing elements: {len(diff_locations[0])}\\n\"\n                       f\"First few differences:\\n\"\n                       f\"a: {a[diff_locations][:5]}\\n\"\n                       f\"b: {b[diff_locations][:5]}\")\nE           Failed: Tensors not close!\nE           Max difference: 1.8267240524291992\nE           Mean difference: 0.22165079414844513\nE           Number of differing elements: 488\nE           First few differences:\nE           a: tensor([  0.3062,  -5.5643, -20.5194, -11.3056,  -8.0638], device='cuda:0')\nE           b: tensor([  0.1106,  -5.3961, -20.8034, -11.4259,  -8.6405], device='cuda:0')\n\nneuron_cupy/test_snn_operator.py:50: Failed\n=========================== short test summary info ============================\nFAILED neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[True-dtype0]\nFAILED neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[False-dtype1]\nFAILED neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[True-dtype2]\nFAILED neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[False-dtype3]\n========================= 4 failed, 13 passed in 1.02s =========================\n",
      "all_tests_passed": false
    },
    "memory_usage": {
      "gpu_memory": {
        "allocated": 0,
        "cached": 0,
        "max_allocated": 0,
        "device_name": "NVIDIA GeForce RTX 3090"
      },
      "system_memory": {
        "total": 67199180800,
        "available": 58820530176,
        "percent": 12.5
      },
      "timestamp": "2025-06-09T16:04:45.129382"
    }
  }
}