ST-BIF Baseline Snapshot
生成时间: 2025-06-09T16:04:45.129398
Git提交: b2f8072c
Git分支: ci-infrastructure
CUDA可用: True
GPU: NVIDIA GeForce RTX 3090

=== MODEL_ACCURACY ===
ann_accuracy: 86.74
qann_accuracy: 85.17
snn_accuracy: 85.12
note: 默认值，需要手动验证
conversion_success: True
execution_time: 600s

=== CUDA_KERNELS ===
error: CUDA基准测试失败: Traceback (most recent call last):
  File "/home/zilingwei/Projects/ST-BIF_Profiling/profile/scripts/cuda_kernel_benchmark.py", line 29, in <module>
    from neuron_cupy.cuda_operator import ST_BIFNodeATGF_MS_CUDA
ModuleNotFoundError: No module named 'neuron_cupy'

benchmark_success: False

=== CUDA_EQUIVALENCE ===
test_success: True
execution_time: 180s
test_output: ============================= test session starts ==============================
platform linux -- Python 3.12.2, pytest-8.3.3, pluggy-1.5.0 -- /home/zilingwei/miniconda3/bin/python
cachedir: .pytest_cache
rootdir: /home/zilingwei/Projects/ST-BIF_Profiling
collecting ... collected 17 items

neuron_cupy/test_snn_operator.py::TestSNNOperator::test_forward_pass[True-dtype0] PASSED [  5%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_forward_pass[False-dtype1] PASSED [ 11%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_forward_pass[True-dtype2] PASSED [ 17%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_forward_pass[False-dtype3] PASSED [ 23%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[True-dtype0] FAILED [ 29%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[False-dtype1] FAILED [ 35%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[True-dtype2] FAILED [ 41%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[False-dtype3] FAILED [ 47%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[1-1-dtype0] PASSED [ 52%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[32-32-dtype1] PASSED [ 58%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[128-64-dtype2] PASSED [ 64%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[256-128-dtype3] PASSED [ 70%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[1-1-dtype4] PASSED [ 76%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[32-32-dtype5] PASSED [ 82%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[128-64-dtype6] PASSED [ 88%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_varying_sizes[256-128-dtype7] PASSED [ 94%]
neuron_cupy/test_snn_operator.py::TestSNNOperator::test_edge_cases PASSED [100%]

=================================== FAILURES ===================================
_______________ TestSNNOperator.test_backward_pass[True-dtype0] ________________

self = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2f3e0>
test_shapes = [(10, 2, 32), (5, 16, 64), (20, 1, 16), (15, 8, 128), (32, 4, 75264), (128, 4, 1024)]
threshold_values = (tensor(0.1000, device='cuda:0'), tensor(4., device='cuda:0'), tensor(-4., device='cuda:0'))
device = device(type='cuda'), use_seed = True, dtype = torch.float16

    @pytest.mark.parametrize("use_seed, dtype",
        [(True, torch.float16), (False, torch.float16), (True, torch.float32), (False, torch.float32)]
    )
    def test_backward_pass(self, test_shapes, threshold_values, device, use_seed, dtype):
        """Test backward pass gradient consistency"""
        if use_seed:
            torch.manual_seed(42)
    
        v_th, T_max, T_min = threshold_values
        v_th = v_th.type(dtype)
        T_max = T_max.type(dtype)
        T_min = T_min.type(dtype)
        prefire = (v_th * 0.0).type(dtype)
    
        for shape in test_shapes:
            # Generate input
            x = self.generate_input_data(shape, device, dtype)
            x_copy = x.clone().detach().requires_grad_(True)
    
            # Forward pass
            pytorch_op = ST_BIFNodeATGF_MS.apply
            cuda_op = ST_BIFNodeATGF_MS_CUDA.apply
    
            spike_seq_pt, v_pt, T_seq_pt = pytorch_op(x, v_th, T_max, T_min, prefire)
            spike_seq_cuda, v_cuda, T_seq_cuda = cuda_op(x_copy, v_th, T_max, T_min, prefire)
    
            # Generate gradient tensors
            grad_spike = torch.randn_like(spike_seq_pt)
            grad_v = torch.randn_like(v_pt)
            grad_T = torch.randn_like(T_seq_pt)
    
            # Backward pass
            spike_seq_pt.backward(grad_spike, retain_graph=True)
            v_pt.backward(grad_v, retain_graph=True)
            T_seq_pt.backward(grad_T)
    
            spike_seq_cuda.backward(grad_spike, retain_graph=True)
            v_cuda.backward(grad_v, retain_graph=True)
            T_seq_cuda.backward(grad_T)
    
            # Compare gradients
            self.assert_tensor_close(spike_seq_pt, spike_seq_cuda, rtol=1e-3, atol=1e-4)
>           self.assert_tensor_close(x.grad, x_copy.grad, rtol=1e-3, atol=1e-4)

neuron_cupy/test_snn_operator.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2f3e0>
a = tensor([[[ 1.4722e-01, -5.1904e-01,  7.6680e+00,  3.8281e-01,  3.0410e+00,
          -9.2422e+00, -5.3662e-01,  2.9190...00,  0.0000e+00,  0.0000e+00, -3.6865e-02,
          -9.0942e-03, -7.9572e-05]]], device='cuda:0', dtype=torch.float16)
b = tensor([[[ 1.9226e-01, -4.5483e-01,  8.4688e+00,  4.1309e-01,  2.8828e+00,
          -9.8281e+00, -4.3182e-02, -6.8542...00,  0.0000e+00,  0.0000e+00, -4.2419e-03,
          -9.8801e-04,  0.0000e+00]]], device='cuda:0', dtype=torch.float16)
rtol = 0.001, atol = 0.0001

    def assert_tensor_close(self, a: torch.Tensor, b: torch.Tensor, rtol=1e-5, atol=1e-6):
        """Custom assertion for tensor comparison with detailed error message"""
        if not torch.allclose(a, b, rtol=rtol, atol=atol):
            max_diff = torch.max(torch.abs(a - b)).item()
            mean_diff = torch.mean(torch.abs(a - b)).item()
            diff_locations = torch.where(torch.abs(a - b) > atol)
>           pytest.fail(f"Tensors not close!\nMax difference: {max_diff}\n"
                       f"Mean difference: {mean_diff}\n"
                       f"Number of differing elements: {len(diff_locations[0])}\n"
                       f"First few differences:\n"
                       f"a: {a[diff_locations][:5]}\n"
                       f"b: {b[diff_locations][:5]}")
E           Failed: Tensors not close!
E           Max difference: 2.6171875
E           Mean difference: 0.2254638671875
E           Number of differing elements: 469
E           First few differences:
E           a: tensor([ 0.1472, -0.5190,  7.6680,  0.3828,  3.0410], device='cuda:0',
E                  dtype=torch.float16)
E           b: tensor([ 0.1923, -0.4548,  8.4688,  0.4131,  2.8828], device='cuda:0',
E                  dtype=torch.float16)

neuron_cupy/test_snn_operator.py:50: Failed
_______________ TestSNNOperator.test_backward_pass[False-dtype1] _______________

self = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2ef90>
test_shapes = [(10, 2, 32), (5, 16, 64), (20, 1, 16), (15, 8, 128), (32, 4, 75264), (128, 4, 1024)]
threshold_values = (tensor(0.1000, device='cuda:0'), tensor(4., device='cuda:0'), tensor(-4., device='cuda:0'))
device = device(type='cuda'), use_seed = False, dtype = torch.float16

    @pytest.mark.parametrize("use_seed, dtype",
        [(True, torch.float16), (False, torch.float16), (True, torch.float32), (False, torch.float32)]
    )
    def test_backward_pass(self, test_shapes, threshold_values, device, use_seed, dtype):
        """Test backward pass gradient consistency"""
        if use_seed:
            torch.manual_seed(42)
    
        v_th, T_max, T_min = threshold_values
        v_th = v_th.type(dtype)
        T_max = T_max.type(dtype)
        T_min = T_min.type(dtype)
        prefire = (v_th * 0.0).type(dtype)
    
        for shape in test_shapes:
            # Generate input
            x = self.generate_input_data(shape, device, dtype)
            x_copy = x.clone().detach().requires_grad_(True)
    
            # Forward pass
            pytorch_op = ST_BIFNodeATGF_MS.apply
            cuda_op = ST_BIFNodeATGF_MS_CUDA.apply
    
            spike_seq_pt, v_pt, T_seq_pt = pytorch_op(x, v_th, T_max, T_min, prefire)
            spike_seq_cuda, v_cuda, T_seq_cuda = cuda_op(x_copy, v_th, T_max, T_min, prefire)
    
            # Generate gradient tensors
            grad_spike = torch.randn_like(spike_seq_pt)
            grad_v = torch.randn_like(v_pt)
            grad_T = torch.randn_like(T_seq_pt)
    
            # Backward pass
            spike_seq_pt.backward(grad_spike, retain_graph=True)
            v_pt.backward(grad_v, retain_graph=True)
            T_seq_pt.backward(grad_T)
    
            spike_seq_cuda.backward(grad_spike, retain_graph=True)
            v_cuda.backward(grad_v, retain_graph=True)
            T_seq_cuda.backward(grad_T)
    
            # Compare gradients
            self.assert_tensor_close(spike_seq_pt, spike_seq_cuda, rtol=1e-3, atol=1e-4)
>           self.assert_tensor_close(x.grad, x_copy.grad, rtol=1e-3, atol=1e-4)

neuron_cupy/test_snn_operator.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2ef90>
a = tensor([[[ 2.9077e-01, -5.5664e+00, -2.0531e+01, -1.1336e+01, -8.0781e+00,
           8.8516e+00, -7.6445e+00, -1.3223...00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           0.0000e+00,  1.1398e-02]]], device='cuda:0', dtype=torch.float16)
b = tensor([[[ 1.0931e-01, -5.3945e+00, -2.0812e+01, -1.1445e+01, -8.6484e+00,
           9.5625e+00, -8.1719e+00, -8.8281...00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
           0.0000e+00,  1.1742e-05]]], device='cuda:0', dtype=torch.float16)
rtol = 0.001, atol = 0.0001

    def assert_tensor_close(self, a: torch.Tensor, b: torch.Tensor, rtol=1e-5, atol=1e-6):
        """Custom assertion for tensor comparison with detailed error message"""
        if not torch.allclose(a, b, rtol=rtol, atol=atol):
            max_diff = torch.max(torch.abs(a - b)).item()
            mean_diff = torch.mean(torch.abs(a - b)).item()
            diff_locations = torch.where(torch.abs(a - b) > atol)
>           pytest.fail(f"Tensors not close!\nMax difference: {max_diff}\n"
                       f"Mean difference: {mean_diff}\n"
                       f"Number of differing elements: {len(diff_locations[0])}\n"
                       f"First few differences:\n"
                       f"a: {a[diff_locations][:5]}\n"
                       f"b: {b[diff_locations][:5]}")
E           Failed: Tensors not close!
E           Max difference: 1.79296875
E           Mean difference: 0.2215576171875
E           Number of differing elements: 465
E           First few differences:
E           a: tensor([  0.2908,  -5.5664, -20.5312, -11.3359,  -8.0781], device='cuda:0',
E                  dtype=torch.float16)
E           b: tensor([  0.1093,  -5.3945, -20.8125, -11.4453,  -8.6484], device='cuda:0',
E                  dtype=torch.float16)

neuron_cupy/test_snn_operator.py:50: Failed
_______________ TestSNNOperator.test_backward_pass[True-dtype2] ________________

self = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2e7e0>
test_shapes = [(10, 2, 32), (5, 16, 64), (20, 1, 16), (15, 8, 128), (32, 4, 75264), (128, 4, 1024)]
threshold_values = (tensor(0.1000, device='cuda:0'), tensor(4., device='cuda:0'), tensor(-4., device='cuda:0'))
device = device(type='cuda'), use_seed = True, dtype = torch.float32

    @pytest.mark.parametrize("use_seed, dtype",
        [(True, torch.float16), (False, torch.float16), (True, torch.float32), (False, torch.float32)]
    )
    def test_backward_pass(self, test_shapes, threshold_values, device, use_seed, dtype):
        """Test backward pass gradient consistency"""
        if use_seed:
            torch.manual_seed(42)
    
        v_th, T_max, T_min = threshold_values
        v_th = v_th.type(dtype)
        T_max = T_max.type(dtype)
        T_min = T_min.type(dtype)
        prefire = (v_th * 0.0).type(dtype)
    
        for shape in test_shapes:
            # Generate input
            x = self.generate_input_data(shape, device, dtype)
            x_copy = x.clone().detach().requires_grad_(True)
    
            # Forward pass
            pytorch_op = ST_BIFNodeATGF_MS.apply
            cuda_op = ST_BIFNodeATGF_MS_CUDA.apply
    
            spike_seq_pt, v_pt, T_seq_pt = pytorch_op(x, v_th, T_max, T_min, prefire)
            spike_seq_cuda, v_cuda, T_seq_cuda = cuda_op(x_copy, v_th, T_max, T_min, prefire)
    
            # Generate gradient tensors
            grad_spike = torch.randn_like(spike_seq_pt)
            grad_v = torch.randn_like(v_pt)
            grad_T = torch.randn_like(T_seq_pt)
    
            # Backward pass
            spike_seq_pt.backward(grad_spike, retain_graph=True)
            v_pt.backward(grad_v, retain_graph=True)
            T_seq_pt.backward(grad_T)
    
            spike_seq_cuda.backward(grad_spike, retain_graph=True)
            v_cuda.backward(grad_v, retain_graph=True)
            T_seq_cuda.backward(grad_T)
    
            # Compare gradients
            self.assert_tensor_close(spike_seq_pt, spike_seq_cuda, rtol=1e-3, atol=1e-4)
>           self.assert_tensor_close(x.grad, x_copy.grad, rtol=1e-3, atol=1e-4)

neuron_cupy/test_snn_operator.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2e7e0>
a = tensor([[[ 1.4263e-01, -5.3339e-01,  7.6693e+00,  3.7072e-01,  3.0406e+00,
          -9.2475e+00, -5.0615e-01, -6.7558...1.3283e-04, -1.8654e-18, -3.0695e-14,  2.4870e-13, -3.5315e-02,
          -8.4628e-03, -7.8368e-05]]], device='cuda:0')
b = tensor([[[ 1.9161e-01, -4.6682e-01,  8.4800e+00,  3.9843e-01,  2.8732e+00,
          -9.8201e+00, -1.6368e-02, -1.2154...4.0847e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.3326e-03,
          -1.0003e-03, -7.4741e-12]]], device='cuda:0')
rtol = 0.001, atol = 0.0001

    def assert_tensor_close(self, a: torch.Tensor, b: torch.Tensor, rtol=1e-5, atol=1e-6):
        """Custom assertion for tensor comparison with detailed error message"""
        if not torch.allclose(a, b, rtol=rtol, atol=atol):
            max_diff = torch.max(torch.abs(a - b)).item()
            mean_diff = torch.mean(torch.abs(a - b)).item()
            diff_locations = torch.where(torch.abs(a - b) > atol)
>           pytest.fail(f"Tensors not close!\nMax difference: {max_diff}\n"
                       f"Mean difference: {mean_diff}\n"
                       f"Number of differing elements: {len(diff_locations[0])}\n"
                       f"First few differences:\n"
                       f"a: {a[diff_locations][:5]}\n"
                       f"b: {b[diff_locations][:5]}")
E           Failed: Tensors not close!
E           Max difference: 2.642458915710449
E           Mean difference: 0.22469036281108856
E           Number of differing elements: 485
E           First few differences:
E           a: tensor([ 0.1426, -0.5334,  7.6693,  0.3707,  3.0406], device='cuda:0')
E           b: tensor([ 0.1916, -0.4668,  8.4800,  0.3984,  2.8732], device='cuda:0')

neuron_cupy/test_snn_operator.py:50: Failed
_______________ TestSNNOperator.test_backward_pass[False-dtype3] _______________

self = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2eae0>
test_shapes = [(10, 2, 32), (5, 16, 64), (20, 1, 16), (15, 8, 128), (32, 4, 75264), (128, 4, 1024)]
threshold_values = (tensor(0.1000, device='cuda:0'), tensor(4., device='cuda:0'), tensor(-4., device='cuda:0'))
device = device(type='cuda'), use_seed = False, dtype = torch.float32

    @pytest.mark.parametrize("use_seed, dtype",
        [(True, torch.float16), (False, torch.float16), (True, torch.float32), (False, torch.float32)]
    )
    def test_backward_pass(self, test_shapes, threshold_values, device, use_seed, dtype):
        """Test backward pass gradient consistency"""
        if use_seed:
            torch.manual_seed(42)
    
        v_th, T_max, T_min = threshold_values
        v_th = v_th.type(dtype)
        T_max = T_max.type(dtype)
        T_min = T_min.type(dtype)
        prefire = (v_th * 0.0).type(dtype)
    
        for shape in test_shapes:
            # Generate input
            x = self.generate_input_data(shape, device, dtype)
            x_copy = x.clone().detach().requires_grad_(True)
    
            # Forward pass
            pytorch_op = ST_BIFNodeATGF_MS.apply
            cuda_op = ST_BIFNodeATGF_MS_CUDA.apply
    
            spike_seq_pt, v_pt, T_seq_pt = pytorch_op(x, v_th, T_max, T_min, prefire)
            spike_seq_cuda, v_cuda, T_seq_cuda = cuda_op(x_copy, v_th, T_max, T_min, prefire)
    
            # Generate gradient tensors
            grad_spike = torch.randn_like(spike_seq_pt)
            grad_v = torch.randn_like(v_pt)
            grad_T = torch.randn_like(T_seq_pt)
    
            # Backward pass
            spike_seq_pt.backward(grad_spike, retain_graph=True)
            v_pt.backward(grad_v, retain_graph=True)
            T_seq_pt.backward(grad_T)
    
            spike_seq_cuda.backward(grad_spike, retain_graph=True)
            v_cuda.backward(grad_v, retain_graph=True)
            T_seq_cuda.backward(grad_T)
    
            # Compare gradients
            self.assert_tensor_close(spike_seq_pt, spike_seq_cuda, rtol=1e-3, atol=1e-4)
>           self.assert_tensor_close(x.grad, x_copy.grad, rtol=1e-3, atol=1e-4)

neuron_cupy/test_snn_operator.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_snn_operator.TestSNNOperator object at 0x7fdb08b2eae0>
a = tensor([[[ 3.0624e-01, -5.5643e+00, -2.0519e+01, -1.1306e+01, -8.0638e+00,
           8.9385e+00, -7.7033e+00, -1.3309...1.3607e-04,  7.0414e+00,  4.5662e-07,  2.5813e-11,  1.1370e-08,
          -1.0402e-15,  5.8459e-03]]], device='cuda:0')
b = tensor([[[ 1.1064e-01, -5.3961e+00, -2.0803e+01, -1.1426e+01, -8.6405e+00,
           9.6620e+00, -8.2066e+00, -8.7231...1.2671e-11,  6.8953e+00,  9.6921e-21,  0.0000e+00,  0.0000e+00,
           0.0000e+00,  1.1493e-05]]], device='cuda:0')
rtol = 0.001, atol = 0.0001

    def assert_tensor_close(self, a: torch.Tensor, b: torch.Tensor, rtol=1e-5, atol=1e-6):
        """Custom assertion for tensor comparison with detailed error message"""
        if not torch.allclose(a, b, rtol=rtol, atol=atol):
            max_diff = torch.max(torch.abs(a - b)).item()
            mean_diff = torch.mean(torch.abs(a - b)).item()
            diff_locations = torch.where(torch.abs(a - b) > atol)
>           pytest.fail(f"Tensors not close!\nMax difference: {max_diff}\n"
                       f"Mean difference: {mean_diff}\n"
                       f"Number of differing elements: {len(diff_locations[0])}\n"
                       f"First few differences:\n"
                       f"a: {a[diff_locations][:5]}\n"
                       f"b: {b[diff_locations][:5]}")
E           Failed: Tensors not close!
E           Max difference: 1.8267240524291992
E           Mean difference: 0.22165079414844513
E           Number of differing elements: 488
E           First few differences:
E           a: tensor([  0.3062,  -5.5643, -20.5194, -11.3056,  -8.0638], device='cuda:0')
E           b: tensor([  0.1106,  -5.3961, -20.8034, -11.4259,  -8.6405], device='cuda:0')

neuron_cupy/test_snn_operator.py:50: Failed
=========================== short test summary info ============================
FAILED neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[True-dtype0]
FAILED neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[False-dtype1]
FAILED neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[True-dtype2]
FAILED neuron_cupy/test_snn_operator.py::TestSNNOperator::test_backward_pass[False-dtype3]
========================= 4 failed, 13 passed in 1.02s =========================

all_tests_passed: False

=== MEMORY_USAGE ===
gpu_memory: {'allocated': 0, 'cached': 0, 'max_allocated': 0, 'device_name': 'NVIDIA GeForce RTX 3090'}
system_memory: {'total': 67199180800, 'available': 58820530176, 'percent': 12.5}
timestamp: 2025-06-09T16:04:45.129382

