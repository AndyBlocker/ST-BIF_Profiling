#!/usr/bin/env python3
"""
æ”¹è¿›çš„è½¬æ¢æµ‹è¯•è„šæœ¬ - ä½¿ç”¨æ›´åˆç†çš„é‡åŒ–å‚æ•°
"""

import torch
import torch.nn as nn
import copy
from torchvision import datasets, transforms
from tqdm import tqdm
import time

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from spike_quan_wrapper_ICML import SNNWrapper_MS, myquan_replace_resnet
import resnet


def build_test_dataset():
    """æ„å»ºæµ‹è¯•æ•°æ®é›†"""
    mean = [0.4914, 0.4822, 0.4465]
    std = [0.2023, 0.1994, 0.2010]
    
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
    
    testset = datasets.CIFAR10(root='/home/zilingwei/cifar10', train=False, download=True, transform=test_transform)
    return testset


def create_test_dataloader(dataset, batch_size=128):
    """åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    test_loader = torch.utils.data.DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=4, 
        pin_memory=torch.cuda.is_available()
    )
    return test_loader


def test_model(model, test_loader, model_name="Model", max_batches=None):
    """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    
    correct = 0
    total = 0
    total_time = 0
    
    print(f"ğŸ§ª æµ‹è¯•{model_name}æ¨¡å‹...")
    
    with torch.no_grad():
        test_pbar = tqdm(test_loader, desc=f"æµ‹è¯•{model_name}", unit="batch")
        
        for batch_idx, (data, target) in enumerate(test_pbar):
            if max_batches and batch_idx >= max_batches:
                break
                
            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
            
            # è®¡ç®—æ¨ç†æ—¶é—´
            torch.cuda.synchronize()
            start_time = time.time()
            output = model(data)
            torch.cuda.synchronize()
            inference_time = time.time() - start_time
            total_time += inference_time
            
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            current_acc = 100. * correct / total
            test_pbar.set_postfix({
                'Acc': f'{current_acc:.2f}%',
                'Time': f'{inference_time*1000:.1f}ms'
            })
    
    accuracy = 100. * correct / total
    num_batches = max_batches if max_batches else len(test_loader)
    avg_time_per_batch = total_time / min(num_batches, len(test_loader))
    avg_time_per_sample = total_time / total
    
    print(f"âœ… {model_name}æµ‹è¯•å®Œæˆ:")
    print(f"   ğŸ“Š å‡†ç¡®ç‡: {accuracy:.2f}%")
    print(f"   â±ï¸  å¹³å‡æ¯batchæ—¶é—´: {avg_time_per_batch*1000:.2f}ms")
    print(f"   ğŸš€ å¹³å‡æ¯æ ·æœ¬æ—¶é—´: {avg_time_per_sample*1000:.3f}ms")
    
    return accuracy, avg_time_per_sample


def load_model_weights(model, model_path, model_name):
    """åŠ è½½æ¨¡å‹æƒé‡"""
    try:
        print(f"ğŸ“‚ åŠ è½½{model_name}æ¨¡å‹: {model_path}")
        checkpoint = torch.load(model_path, map_location='cpu')
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print(f"âœ… {model_name}æ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ åŠ è½½{model_name}æ¨¡å‹å¤±è´¥: {e}")
        return False


def test_different_quantization_levels(test_loader, ann_model_path="best_ANN.pth"):
    """æµ‹è¯•ä¸åŒé‡åŒ–çº§åˆ«çš„æ•ˆæœ"""
    print("\n" + "="*70)
    print("ğŸ”¬ æ”¹è¿›çš„é‡åŒ–æµ‹è¯•ï¼šä¸åŒLevelå¯¹æ¯”")
    print("="*70)
    
    # åŠ è½½åŸºç¡€ANNæ¨¡å‹
    ann_model = resnet.resnet18(pretrained=False)
    ann_model.fc = torch.nn.Linear(ann_model.fc.in_features, 10)
    
    if not load_model_weights(ann_model, ann_model_path, "ANN"):
        return None
    
    results = {}
    
    # æµ‹è¯•åŸå§‹ANN
    print("\nğŸ§  æµ‹è¯•åŸå§‹ANNæ¨¡å‹")
    ann_accuracy, ann_time = test_model(ann_model, test_loader, "ANN", max_batches=20)
    results['ANN'] = {"accuracy": ann_accuracy, "inference_time": ann_time}
    
    # æµ‹è¯•ä¸åŒçš„é‡åŒ–çº§åˆ«
    levels = [4, 8, 16]  # ä½¿ç”¨æ›´æ¸©å’Œçš„é‡åŒ–çº§åˆ«
    
    for level in levels:
        print(f"\nâš¡ æµ‹è¯•Level {level}é‡åŒ–")
        
        # æ·±åº¦å¤åˆ¶æ¨¡å‹
        qann_model = copy.deepcopy(ann_model)
        
        # åº”ç”¨é‡åŒ–
        print(f"ğŸ”„ åº”ç”¨Level {level}é‡åŒ–...")
        myquan_replace_resnet(qann_model, level=level, weight_bit=32, is_softmax=False)
        
        # æµ‹è¯•é‡åŒ–æ¨¡å‹
        qann_accuracy, qann_time = test_model(qann_model, test_loader, f"QANN-L{level}", max_batches=20)
        results[f'QANN-L{level}'] = {"accuracy": qann_accuracy, "inference_time": qann_time}
        
        # æ¸…ç†å†…å­˜
        del qann_model
        torch.cuda.empty_cache()
    
    return results


def test_optimized_conversion_pipeline(test_loader, ann_model_path="best_ANN.pth"):
    """
    ä¼˜åŒ–çš„è½¬æ¢æµ‹è¯•ï¼šANN â†’ QANN(åˆç†level) â†’ SNN
    """
    print("\n" + "="*70)
    print("ğŸš€ ä¼˜åŒ–çš„è½¬æ¢æµ‹è¯•æµæ°´çº¿: ANN â†’ QANN(L32) â†’ SNN")
    print("="*70)
    
    # åŠ è½½åŸºç¡€ANNæ¨¡å‹
    ann_model = resnet.resnet18(pretrained=False)
    ann_model.fc = torch.nn.Linear(ann_model.fc.in_features, 10)
    
    if not load_model_weights(ann_model, ann_model_path, "ANN"):
        return None
    
    results = {}
    
    # æµ‹è¯•åŸå§‹ANN
    print("\nğŸ§  æ­¥éª¤ 1/3: æµ‹è¯•ANNæ¨¡å‹")
    ann_accuracy, ann_time = test_model(ann_model, test_loader, "ANN", max_batches=30)
    results['ANN'] = {"accuracy": ann_accuracy, "inference_time": ann_time}
    
    # è½¬æ¢ä¸ºQANN (ä½¿ç”¨æ›´åˆç†çš„level=32)
    print("\nğŸ”„ æ­¥éª¤ 2/3: è½¬æ¢ä¸ºQANN (Level 32)")
    qann_model = copy.deepcopy(ann_model)
    myquan_replace_resnet(qann_model, level=16, weight_bit=32, is_softmax=False)
    
    print("\nâš¡ æµ‹è¯•QANNæ¨¡å‹")
    qann_accuracy, qann_time = test_model(qann_model, test_loader, "QANN", max_batches=30)
    results['QANN'] = {"accuracy": qann_accuracy, "inference_time": qann_time}
    
    # è½¬æ¢ä¸ºSNN
    print("\nğŸ”„ æ­¥éª¤ 3/3: è½¬æ¢ä¸ºSNN")
    import os
    output_dir = "/home/zilingwei/output_bin_snn_resnet_w32_a4_T8/"
    os.makedirs(output_dir, exist_ok=True)
    
    snn_model = SNNWrapper_MS(
        ann_model=qann_model,
        cfg=None, 
        time_step=8,
        Encoding_type="analog", 
        level=16,  # ä¸QANNä¿æŒä¸€è‡´
        neuron_type="ST-BIF",
        model_name="resnet", 
        is_softmax=False,  
        suppress_over_fire=False,
        record_inout=False,
        learnable=True,
        record_dir=output_dir
    )
    
    print("\nğŸ§  æµ‹è¯•SNNæ¨¡å‹")
    snn_accuracy, snn_time = test_model(snn_model, test_loader, "SNN", max_batches=30)
    results['SNN'] = {"accuracy": snn_accuracy, "inference_time": snn_time}
    
    return results


def compare_results(results):
    """æ¯”è¾ƒç»“æœ"""
    print("\n" + "="*80)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ")
    print("="*80)
    
    print(f"{'æ¨¡å‹ç±»å‹':<15} {'å‡†ç¡®ç‡':<12} {'æ¨ç†æ—¶é—´':<15} {'ç›¸å¯¹é€Ÿåº¦':<12} {'å‡†ç¡®ç‡å˜åŒ–':<12}")
    print("-" * 80)
    
    # ä»¥ANNä¸ºåŸºçº¿
    baseline_acc = None
    baseline_time = None
    
    if 'ANN' in results and results['ANN'] is not None:
        baseline_acc = results['ANN']['accuracy']
        baseline_time = results['ANN']['inference_time']
    
    for model_type, result in results.items():
        if result is not None:
            acc = result['accuracy']
            time_ms = result['inference_time'] * 1000
            
            # è®¡ç®—ç›¸å¯¹é€Ÿåº¦
            if baseline_time:
                speed_ratio = baseline_time / result['inference_time']
                speed_str = f"{speed_ratio:.2f}x"
            else:
                speed_str = "baseline"
            
            # è®¡ç®—å‡†ç¡®ç‡å˜åŒ–
            if baseline_acc and model_type != 'ANN':
                acc_change = acc - baseline_acc
                acc_change_str = f"{acc_change:+.2f}%"
            else:
                acc_change_str = "baseline"
            
            print(f"{model_type:<15} {acc:<11.2f}% {time_ms:<13.3f}ms {speed_str:<12} {acc_change_str:<12}")
    
    print("-" * 80)


def main():
    print("ğŸ¯ æ”¹è¿›çš„é‡åŒ–è½¬æ¢æµ‹è¯•")
    print(f"ğŸ“± è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    
    # å‡†å¤‡æ•°æ®
    print("\nğŸ“Š å‡†å¤‡æµ‹è¯•æ•°æ®...")
    test_set = build_test_dataset()
    test_loader = create_test_dataloader(test_set, 128)
    print(f"âœ… æµ‹è¯•é›†å¤§å°: {len(test_set)} æ ·æœ¬")
    
    # æµ‹è¯•1: ä¸åŒé‡åŒ–çº§åˆ«å¯¹æ¯”
    print("\nğŸ”¬ æµ‹è¯•ä¸åŒé‡åŒ–çº§åˆ«...")
    level_results = test_different_quantization_levels(test_loader)
    if level_results:
        compare_results(level_results)
    
    # æµ‹è¯•2: ä¼˜åŒ–çš„è½¬æ¢æµæ°´çº¿
    print("\nğŸš€ æµ‹è¯•ä¼˜åŒ–çš„è½¬æ¢æµæ°´çº¿...")
    pipeline_results = test_optimized_conversion_pipeline(test_loader)
    if pipeline_results:
        compare_results(pipeline_results)
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()